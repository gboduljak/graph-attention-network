{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrEINMv8xrrb",
        "outputId": "7c56cf29-d7fb-4255-8742-231f7d767cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "# Check PyTorch version installed on this system\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AUKBJ8Z0neS",
        "outputId": "a4ff2972-0b31-43a4-f8a7-27241b3aed69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Dec 15 01:20:30 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P0    30W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uWUAAH_XxxWC",
        "outputId": "184cbf42-eeee-44e6-fdad-57b594f4ca94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TORCH=1.13.0+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 14.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.15%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 24.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 78.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=a06a7a2134161f7a1a5780e9f1c8c793f251a4d7a5fe79ff8731f9bc4fa31ccd\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 36.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (2.8.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "# Download the required modules\n",
        "\"\"\"\n",
        "Assign to TORCH with what you get from the cell above, E.g., export TORCH=1.13.0+cu116\n",
        "\"\"\"\n",
        "%env TORCH=1.13.0+cu116\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric\n",
        "!pip install torchmetrics\n",
        "!pip install matplotlib\n",
        "!pip install networkx\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaHOK91EyGuR",
        "outputId": "fda1ab70-387e-4d16-d03f-9077c4b4e00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: github_repository=grl\n"
          ]
        }
      ],
      "source": [
        "github_username=\"deeplearningtester\"\n",
        "github_repository=\"grl\"\n",
        "github_token = \"\"\n",
        "\n",
        "%env github_repository={github_repository}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWUxyUz_x8et",
        "outputId": "7d798116-9e70-4975-d524-9af899f4bc21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'grl'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 28 (delta 5), reused 24 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://{github_token}@github.com/{github_username}/{github_repository}.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtuxeTnU17rM",
        "outputId": "4ef27b51-5013-457f-ee87-ee5e7324719a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/grl\n"
          ]
        }
      ],
      "source": [
        "%cd $github_repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6SbBZVV0Ek6",
        "outputId": "e47c2eae-993a-48d8-ee40-50afacbbbd3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f81766da5b0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohY0SVKP0Fb5",
        "outputId": "7bef52a2-5916-45ba-9be5-3c3d7182c1a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting ./ppi.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import PPI\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.transforms import AddSelfLoops\n",
        "import dataset\n",
        "\n",
        "train_dataset = PPI(root='', split='train', transform=AddSelfLoops())\n",
        "val_dataset = PPI(root='', split='val', transform=AddSelfLoops())\n",
        "test_dataset = PPI(root='', split='test', transform=AddSelfLoops())\n",
        "\n",
        "dataset.num_features = 50\n",
        "dataset.num_labels = 121\n",
        "dataset.train_loader = DataLoader(train_dataset, batch_size=2)\n",
        "dataset.val_loader = DataLoader(val_dataset, batch_size=2)\n",
        "dataset.test_loader = DataLoader(test_dataset, batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IXxawUNr0Hnp"
      },
      "outputs": [],
      "source": [
        "from evaluation import evaluate\n",
        "from training_loop import train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXAdnAV20MvV",
        "outputId": "4ba0e368-5362-49c5-929d-4dc2d4075df5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.7062, device='cuda:0'), tensor(0.3849, device='cuda:0'))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models import GATv1PPI\n",
        "model = GATv1PPI(dataset.num_features, dataset.num_labels)\n",
        "evaluate(model, dataset.test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OAupMVsO0Oup"
      },
      "outputs": [],
      "source": [
        "ppi_train_params = {\n",
        "  \"lr\": 5e-3,\n",
        "  \"weight_decay\": 0,\n",
        "  \"epochs\": 400,\n",
        "  \"patience\": 100,\n",
        "  \"model_name\": model.model_name\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FG6MKVHQ0RC8",
        "outputId": "712931b4-de0e-4eba-f7d2-be878d0e4fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GATv1PPI\n",
            "GATv1PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0): GATLayer()\n",
            "    (1): GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "epoch 00000\n",
            "\ttrain_loss: 0.7257 | train_micro_f1: 0.4291\n",
            "\tval_loss: 0.5745 | val_micro_f1: 0.4004\n",
            "epoch 00001\n",
            "\ttrain_loss: 0.5554 | train_micro_f1: 0.4628\n",
            "\tval_loss: 0.5160 | val_micro_f1: 0.4577\n",
            "epoch 00002\n",
            "\ttrain_loss: 0.5193 | train_micro_f1: 0.4950\n",
            "\tval_loss: 0.5008 | val_micro_f1: 0.4926\n",
            "epoch 00003\n",
            "\ttrain_loss: 0.5090 | train_micro_f1: 0.5198\n",
            "\tval_loss: 0.4935 | val_micro_f1: 0.4769\n",
            "epoch 00004\n",
            "\ttrain_loss: 0.4998 | train_micro_f1: 0.5242\n",
            "\tval_loss: 0.4896 | val_micro_f1: 0.5447\n",
            "epoch 00005\n",
            "\ttrain_loss: 0.4905 | train_micro_f1: 0.5425\n",
            "\tval_loss: 0.4805 | val_micro_f1: 0.5414\n",
            "epoch 00006\n",
            "\ttrain_loss: 0.4810 | train_micro_f1: 0.5609\n",
            "\tval_loss: 0.4724 | val_micro_f1: 0.5490\n",
            "epoch 00007\n",
            "\ttrain_loss: 0.4720 | train_micro_f1: 0.5737\n",
            "\tval_loss: 0.4705 | val_micro_f1: 0.5832\n",
            "epoch 00008\n",
            "\ttrain_loss: 0.4654 | train_micro_f1: 0.5844\n",
            "\tval_loss: 0.4738 | val_micro_f1: 0.6084\n",
            "epoch 00009\n",
            "\ttrain_loss: 0.4659 | train_micro_f1: 0.5910\n",
            "\tval_loss: 0.4530 | val_micro_f1: 0.5650\n",
            "epoch 00010\n",
            "\ttrain_loss: 0.4546 | train_micro_f1: 0.6021\n",
            "\tval_loss: 0.4439 | val_micro_f1: 0.6120\n",
            "epoch 00011\n",
            "\ttrain_loss: 0.4357 | train_micro_f1: 0.6266\n",
            "\tval_loss: 0.4315 | val_micro_f1: 0.6129\n",
            "epoch 00012\n",
            "\ttrain_loss: 0.4214 | train_micro_f1: 0.6435\n",
            "\tval_loss: 0.4220 | val_micro_f1: 0.6433\n",
            "epoch 00013\n",
            "\ttrain_loss: 0.4126 | train_micro_f1: 0.6586\n",
            "\tval_loss: 0.4148 | val_micro_f1: 0.6134\n",
            "epoch 00014\n",
            "\ttrain_loss: 0.4173 | train_micro_f1: 0.6511\n",
            "\tval_loss: 0.4195 | val_micro_f1: 0.6042\n",
            "epoch 00015\n",
            "\ttrain_loss: 0.4047 | train_micro_f1: 0.6676\n",
            "\tval_loss: 0.4080 | val_micro_f1: 0.6508\n",
            "epoch 00016\n",
            "\ttrain_loss: 0.3878 | train_micro_f1: 0.6873\n",
            "\tval_loss: 0.3877 | val_micro_f1: 0.6687\n",
            "epoch 00017\n",
            "\ttrain_loss: 0.3687 | train_micro_f1: 0.7093\n",
            "\tval_loss: 0.3755 | val_micro_f1: 0.6696\n",
            "epoch 00018\n",
            "\ttrain_loss: 0.3547 | train_micro_f1: 0.7235\n",
            "\tval_loss: 0.3632 | val_micro_f1: 0.7119\n",
            "epoch 00019\n",
            "\ttrain_loss: 0.3488 | train_micro_f1: 0.7307\n",
            "\tval_loss: 0.3610 | val_micro_f1: 0.6908\n",
            "epoch 00020\n",
            "\ttrain_loss: 0.3367 | train_micro_f1: 0.7438\n",
            "\tval_loss: 0.3427 | val_micro_f1: 0.7247\n",
            "epoch 00021\n",
            "\ttrain_loss: 0.3251 | train_micro_f1: 0.7557\n",
            "\tval_loss: 0.3422 | val_micro_f1: 0.7162\n",
            "epoch 00022\n",
            "\ttrain_loss: 0.3108 | train_micro_f1: 0.7714\n",
            "\tval_loss: 0.3377 | val_micro_f1: 0.7188\n",
            "epoch 00023\n",
            "\ttrain_loss: 0.3137 | train_micro_f1: 0.7665\n",
            "\tval_loss: 0.3329 | val_micro_f1: 0.7404\n",
            "epoch 00024\n",
            "\ttrain_loss: 0.3118 | train_micro_f1: 0.7706\n",
            "\tval_loss: 0.3429 | val_micro_f1: 0.7021\n",
            "epoch 00025\n",
            "\ttrain_loss: 0.3111 | train_micro_f1: 0.7704\n",
            "\tval_loss: 0.3243 | val_micro_f1: 0.7509\n",
            "epoch 00026\n",
            "\ttrain_loss: 0.2990 | train_micro_f1: 0.7815\n",
            "\tval_loss: 0.3202 | val_micro_f1: 0.7697\n",
            "epoch 00027\n",
            "\ttrain_loss: 0.2947 | train_micro_f1: 0.7858\n",
            "\tval_loss: 0.3100 | val_micro_f1: 0.7614\n",
            "epoch 00028\n",
            "\ttrain_loss: 0.2764 | train_micro_f1: 0.8043\n",
            "\tval_loss: 0.2934 | val_micro_f1: 0.7803\n",
            "epoch 00029\n",
            "\ttrain_loss: 0.2537 | train_micro_f1: 0.8245\n",
            "\tval_loss: 0.2767 | val_micro_f1: 0.7973\n",
            "epoch 00030\n",
            "\ttrain_loss: 0.2389 | train_micro_f1: 0.8370\n",
            "\tval_loss: 0.2675 | val_micro_f1: 0.8134\n",
            "epoch 00031\n",
            "\ttrain_loss: 0.2276 | train_micro_f1: 0.8471\n",
            "\tval_loss: 0.2600 | val_micro_f1: 0.8193\n",
            "epoch 00032\n",
            "\ttrain_loss: 0.2185 | train_micro_f1: 0.8550\n",
            "\tval_loss: 0.2495 | val_micro_f1: 0.8241\n",
            "epoch 00033\n",
            "\ttrain_loss: 0.2191 | train_micro_f1: 0.8531\n",
            "\tval_loss: 0.2611 | val_micro_f1: 0.8000\n",
            "epoch 00034\n",
            "\ttrain_loss: 0.2321 | train_micro_f1: 0.8404\n",
            "\tval_loss: 0.2624 | val_micro_f1: 0.8100\n",
            "epoch 00035\n",
            "\ttrain_loss: 0.2288 | train_micro_f1: 0.8442\n",
            "\tval_loss: 0.2530 | val_micro_f1: 0.8219\n",
            "epoch 00036\n",
            "\ttrain_loss: 0.2079 | train_micro_f1: 0.8627\n",
            "\tval_loss: 0.2381 | val_micro_f1: 0.8399\n",
            "epoch 00037\n",
            "\ttrain_loss: 0.1898 | train_micro_f1: 0.8785\n",
            "\tval_loss: 0.2231 | val_micro_f1: 0.8487\n",
            "epoch 00038\n",
            "\ttrain_loss: 0.1772 | train_micro_f1: 0.8879\n",
            "\tval_loss: 0.2175 | val_micro_f1: 0.8550\n",
            "epoch 00039\n",
            "\ttrain_loss: 0.1693 | train_micro_f1: 0.8937\n",
            "\tval_loss: 0.2118 | val_micro_f1: 0.8625\n",
            "epoch 00040\n",
            "\ttrain_loss: 0.1668 | train_micro_f1: 0.8949\n",
            "\tval_loss: 0.2116 | val_micro_f1: 0.8639\n",
            "epoch 00041\n",
            "\ttrain_loss: 0.1597 | train_micro_f1: 0.9007\n",
            "\tval_loss: 0.2066 | val_micro_f1: 0.8646\n",
            "epoch 00042\n",
            "\ttrain_loss: 0.1656 | train_micro_f1: 0.8946\n",
            "\tval_loss: 0.2085 | val_micro_f1: 0.8610\n",
            "epoch 00043\n",
            "\ttrain_loss: 0.1610 | train_micro_f1: 0.8979\n",
            "\tval_loss: 0.2066 | val_micro_f1: 0.8636\n",
            "epoch 00044\n",
            "\ttrain_loss: 0.1636 | train_micro_f1: 0.8960\n",
            "\tval_loss: 0.2125 | val_micro_f1: 0.8541\n",
            "epoch 00045\n",
            "\ttrain_loss: 0.1606 | train_micro_f1: 0.8974\n",
            "\tval_loss: 0.2052 | val_micro_f1: 0.8666\n",
            "epoch 00046\n",
            "\ttrain_loss: 0.1688 | train_micro_f1: 0.8893\n",
            "\tval_loss: 0.2080 | val_micro_f1: 0.8677\n",
            "epoch 00047\n",
            "\ttrain_loss: 0.1564 | train_micro_f1: 0.9017\n",
            "\tval_loss: 0.2002 | val_micro_f1: 0.8690\n",
            "epoch 00048\n",
            "\ttrain_loss: 0.1469 | train_micro_f1: 0.9080\n",
            "\tval_loss: 0.1882 | val_micro_f1: 0.8832\n",
            "epoch 00049\n",
            "\ttrain_loss: 0.1377 | train_micro_f1: 0.9157\n",
            "\tval_loss: 0.1879 | val_micro_f1: 0.8805\n",
            "epoch 00050\n",
            "\ttrain_loss: 0.1361 | train_micro_f1: 0.9156\n",
            "\tval_loss: 0.1906 | val_micro_f1: 0.8834\n",
            "epoch 00051\n",
            "\ttrain_loss: 0.1454 | train_micro_f1: 0.9086\n",
            "\tval_loss: 0.1911 | val_micro_f1: 0.8789\n",
            "epoch 00052\n",
            "\ttrain_loss: 0.1398 | train_micro_f1: 0.9126\n",
            "\tval_loss: 0.1925 | val_micro_f1: 0.8820\n",
            "epoch 00053\n",
            "\ttrain_loss: 0.1277 | train_micro_f1: 0.9231\n",
            "\tval_loss: 0.1760 | val_micro_f1: 0.8909\n",
            "epoch 00054\n",
            "\ttrain_loss: 0.1249 | train_micro_f1: 0.9236\n",
            "\tval_loss: 0.1745 | val_micro_f1: 0.8958\n",
            "epoch 00055\n",
            "\ttrain_loss: 0.1174 | train_micro_f1: 0.9297\n",
            "\tval_loss: 0.1671 | val_micro_f1: 0.8986\n",
            "epoch 00056\n",
            "\ttrain_loss: 0.1174 | train_micro_f1: 0.9289\n",
            "\tval_loss: 0.1720 | val_micro_f1: 0.8972\n",
            "epoch 00057\n",
            "\ttrain_loss: 0.1164 | train_micro_f1: 0.9299\n",
            "\tval_loss: 0.1701 | val_micro_f1: 0.8954\n",
            "epoch 00058\n",
            "\ttrain_loss: 0.1117 | train_micro_f1: 0.9329\n",
            "\tval_loss: 0.1684 | val_micro_f1: 0.8985\n",
            "epoch 00059\n",
            "\ttrain_loss: 0.1116 | train_micro_f1: 0.9326\n",
            "\tval_loss: 0.1662 | val_micro_f1: 0.8999\n",
            "epoch 00060\n",
            "\ttrain_loss: 0.1089 | train_micro_f1: 0.9344\n",
            "\tval_loss: 0.1640 | val_micro_f1: 0.9022\n",
            "epoch 00061\n",
            "\ttrain_loss: 0.1088 | train_micro_f1: 0.9345\n",
            "\tval_loss: 0.1635 | val_micro_f1: 0.9026\n",
            "epoch 00062\n",
            "\ttrain_loss: 0.1071 | train_micro_f1: 0.9355\n",
            "\tval_loss: 0.1666 | val_micro_f1: 0.9003\n",
            "epoch 00063\n",
            "\ttrain_loss: 0.1080 | train_micro_f1: 0.9349\n",
            "\tval_loss: 0.1673 | val_micro_f1: 0.8991\n",
            "epoch 00064\n",
            "\ttrain_loss: 0.1105 | train_micro_f1: 0.9324\n",
            "\tval_loss: 0.1706 | val_micro_f1: 0.8987\n",
            "epoch 00065\n",
            "\ttrain_loss: 0.1117 | train_micro_f1: 0.9316\n",
            "\tval_loss: 0.1670 | val_micro_f1: 0.8999\n",
            "epoch 00066\n",
            "\ttrain_loss: 0.1121 | train_micro_f1: 0.9314\n",
            "\tval_loss: 0.1709 | val_micro_f1: 0.8989\n",
            "epoch 00067\n",
            "\ttrain_loss: 0.1081 | train_micro_f1: 0.9344\n",
            "\tval_loss: 0.1608 | val_micro_f1: 0.9052\n",
            "epoch 00068\n",
            "\ttrain_loss: 0.1009 | train_micro_f1: 0.9395\n",
            "\tval_loss: 0.1559 | val_micro_f1: 0.9107\n",
            "epoch 00069\n",
            "\ttrain_loss: 0.0942 | train_micro_f1: 0.9442\n",
            "\tval_loss: 0.1541 | val_micro_f1: 0.9103\n",
            "epoch 00070\n",
            "\ttrain_loss: 0.0900 | train_micro_f1: 0.9470\n",
            "\tval_loss: 0.1486 | val_micro_f1: 0.9160\n",
            "epoch 00071\n",
            "\ttrain_loss: 0.0919 | train_micro_f1: 0.9453\n",
            "\tval_loss: 0.1528 | val_micro_f1: 0.9109\n",
            "epoch 00072\n",
            "\ttrain_loss: 0.0966 | train_micro_f1: 0.9414\n",
            "\tval_loss: 0.1664 | val_micro_f1: 0.9057\n",
            "epoch 00073\n",
            "\ttrain_loss: 0.0962 | train_micro_f1: 0.9427\n",
            "\tval_loss: 0.1509 | val_micro_f1: 0.9125\n",
            "epoch 00074\n",
            "\ttrain_loss: 0.0930 | train_micro_f1: 0.9443\n",
            "\tval_loss: 0.1536 | val_micro_f1: 0.9129\n",
            "epoch 00075\n",
            "\ttrain_loss: 0.0887 | train_micro_f1: 0.9478\n",
            "\tval_loss: 0.1489 | val_micro_f1: 0.9147\n",
            "epoch 00076\n",
            "\ttrain_loss: 0.0890 | train_micro_f1: 0.9470\n",
            "\tval_loss: 0.1503 | val_micro_f1: 0.9142\n",
            "epoch 00077\n",
            "\ttrain_loss: 0.0869 | train_micro_f1: 0.9486\n",
            "\tval_loss: 0.1476 | val_micro_f1: 0.9153\n",
            "epoch 00078\n",
            "\ttrain_loss: 0.0836 | train_micro_f1: 0.9507\n",
            "\tval_loss: 0.1441 | val_micro_f1: 0.9196\n",
            "epoch 00079\n",
            "\ttrain_loss: 0.0819 | train_micro_f1: 0.9518\n",
            "\tval_loss: 0.1419 | val_micro_f1: 0.9210\n",
            "epoch 00080\n",
            "\ttrain_loss: 0.0786 | train_micro_f1: 0.9541\n",
            "\tval_loss: 0.1447 | val_micro_f1: 0.9193\n",
            "epoch 00081\n",
            "\ttrain_loss: 0.0800 | train_micro_f1: 0.9531\n",
            "\tval_loss: 0.1434 | val_micro_f1: 0.9202\n",
            "epoch 00082\n",
            "\ttrain_loss: 0.0802 | train_micro_f1: 0.9528\n",
            "\tval_loss: 0.1425 | val_micro_f1: 0.9219\n",
            "epoch 00083\n",
            "\ttrain_loss: 0.0802 | train_micro_f1: 0.9532\n",
            "\tval_loss: 0.1473 | val_micro_f1: 0.9189\n",
            "epoch 00084\n",
            "\ttrain_loss: 0.0829 | train_micro_f1: 0.9510\n",
            "\tval_loss: 0.1506 | val_micro_f1: 0.9164\n",
            "epoch 00085\n",
            "\ttrain_loss: 0.0878 | train_micro_f1: 0.9477\n",
            "\tval_loss: 0.1472 | val_micro_f1: 0.9186\n",
            "epoch 00086\n",
            "\ttrain_loss: 0.0914 | train_micro_f1: 0.9447\n",
            "\tval_loss: 0.1523 | val_micro_f1: 0.9141\n",
            "epoch 00087\n",
            "\ttrain_loss: 0.0885 | train_micro_f1: 0.9472\n",
            "\tval_loss: 0.1505 | val_micro_f1: 0.9173\n",
            "epoch 00088\n",
            "\ttrain_loss: 0.0883 | train_micro_f1: 0.9471\n",
            "\tval_loss: 0.1510 | val_micro_f1: 0.9158\n",
            "epoch 00089\n",
            "\ttrain_loss: 0.0826 | train_micro_f1: 0.9508\n",
            "\tval_loss: 0.1454 | val_micro_f1: 0.9202\n",
            "epoch 00090\n",
            "\ttrain_loss: 0.0775 | train_micro_f1: 0.9544\n",
            "\tval_loss: 0.1428 | val_micro_f1: 0.9230\n",
            "epoch 00091\n",
            "\ttrain_loss: 0.0702 | train_micro_f1: 0.9594\n",
            "\tval_loss: 0.1349 | val_micro_f1: 0.9285\n",
            "epoch 00092\n",
            "\ttrain_loss: 0.0652 | train_micro_f1: 0.9625\n",
            "\tval_loss: 0.1311 | val_micro_f1: 0.9303\n",
            "epoch 00093\n",
            "\ttrain_loss: 0.0652 | train_micro_f1: 0.9621\n",
            "\tval_loss: 0.1364 | val_micro_f1: 0.9276\n",
            "epoch 00094\n",
            "\ttrain_loss: 0.0680 | train_micro_f1: 0.9600\n",
            "\tval_loss: 0.1332 | val_micro_f1: 0.9281\n",
            "epoch 00095\n",
            "\ttrain_loss: 0.0736 | train_micro_f1: 0.9561\n",
            "\tval_loss: 0.1391 | val_micro_f1: 0.9243\n",
            "epoch 00096\n",
            "\ttrain_loss: 0.0767 | train_micro_f1: 0.9542\n",
            "\tval_loss: 0.1463 | val_micro_f1: 0.9193\n",
            "epoch 00097\n",
            "\ttrain_loss: 0.0807 | train_micro_f1: 0.9514\n",
            "\tval_loss: 0.1438 | val_micro_f1: 0.9222\n",
            "epoch 00098\n",
            "\ttrain_loss: 0.0827 | train_micro_f1: 0.9506\n",
            "\tval_loss: 0.1440 | val_micro_f1: 0.9238\n",
            "epoch 00099\n",
            "\ttrain_loss: 0.0767 | train_micro_f1: 0.9547\n",
            "\tval_loss: 0.1381 | val_micro_f1: 0.9265\n",
            "epoch 00100\n",
            "\ttrain_loss: 0.0748 | train_micro_f1: 0.9558\n",
            "\tval_loss: 0.1329 | val_micro_f1: 0.9292\n",
            "epoch 00101\n",
            "\ttrain_loss: 0.0699 | train_micro_f1: 0.9591\n",
            "\tval_loss: 0.1341 | val_micro_f1: 0.9280\n",
            "epoch 00102\n",
            "\ttrain_loss: 0.0673 | train_micro_f1: 0.9607\n",
            "\tval_loss: 0.1313 | val_micro_f1: 0.9311\n",
            "epoch 00103\n",
            "\ttrain_loss: 0.0650 | train_micro_f1: 0.9622\n",
            "\tval_loss: 0.1328 | val_micro_f1: 0.9308\n",
            "epoch 00104\n",
            "\ttrain_loss: 0.0648 | train_micro_f1: 0.9622\n",
            "\tval_loss: 0.1336 | val_micro_f1: 0.9313\n",
            "epoch 00105\n",
            "\ttrain_loss: 0.0661 | train_micro_f1: 0.9614\n",
            "\tval_loss: 0.1317 | val_micro_f1: 0.9310\n",
            "epoch 00106\n",
            "\ttrain_loss: 0.0659 | train_micro_f1: 0.9619\n",
            "\tval_loss: 0.1335 | val_micro_f1: 0.9291\n",
            "epoch 00107\n",
            "\ttrain_loss: 0.0636 | train_micro_f1: 0.9635\n",
            "\tval_loss: 0.1296 | val_micro_f1: 0.9338\n",
            "epoch 00108\n",
            "\ttrain_loss: 0.0608 | train_micro_f1: 0.9652\n",
            "\tval_loss: 0.1315 | val_micro_f1: 0.9331\n",
            "epoch 00109\n",
            "\ttrain_loss: 0.0574 | train_micro_f1: 0.9683\n",
            "\tval_loss: 0.1237 | val_micro_f1: 0.9373\n",
            "epoch 00110\n",
            "\ttrain_loss: 0.0539 | train_micro_f1: 0.9705\n",
            "\tval_loss: 0.1230 | val_micro_f1: 0.9382\n",
            "epoch 00111\n",
            "\ttrain_loss: 0.0494 | train_micro_f1: 0.9729\n",
            "\tval_loss: 0.1190 | val_micro_f1: 0.9406\n",
            "epoch 00112\n",
            "\ttrain_loss: 0.0449 | train_micro_f1: 0.9755\n",
            "\tval_loss: 0.1160 | val_micro_f1: 0.9420\n",
            "epoch 00113\n",
            "\ttrain_loss: 0.0429 | train_micro_f1: 0.9773\n",
            "\tval_loss: 0.1115 | val_micro_f1: 0.9456\n",
            "epoch 00114\n",
            "\ttrain_loss: 0.0403 | train_micro_f1: 0.9788\n",
            "\tval_loss: 0.1112 | val_micro_f1: 0.9459\n",
            "epoch 00115\n",
            "\ttrain_loss: 0.0381 | train_micro_f1: 0.9801\n",
            "\tval_loss: 0.1119 | val_micro_f1: 0.9457\n",
            "epoch 00116\n",
            "\ttrain_loss: 0.0370 | train_micro_f1: 0.9808\n",
            "\tval_loss: 0.1105 | val_micro_f1: 0.9468\n",
            "epoch 00117\n",
            "\ttrain_loss: 0.0349 | train_micro_f1: 0.9824\n",
            "\tval_loss: 0.1089 | val_micro_f1: 0.9479\n",
            "epoch 00118\n",
            "\ttrain_loss: 0.0341 | train_micro_f1: 0.9826\n",
            "\tval_loss: 0.1100 | val_micro_f1: 0.9483\n",
            "epoch 00119\n",
            "\ttrain_loss: 0.0340 | train_micro_f1: 0.9831\n",
            "\tval_loss: 0.1076 | val_micro_f1: 0.9495\n",
            "epoch 00120\n",
            "\ttrain_loss: 0.0326 | train_micro_f1: 0.9840\n",
            "\tval_loss: 0.1066 | val_micro_f1: 0.9504\n",
            "epoch 00121\n",
            "\ttrain_loss: 0.0317 | train_micro_f1: 0.9846\n",
            "\tval_loss: 0.1071 | val_micro_f1: 0.9494\n",
            "epoch 00122\n",
            "\ttrain_loss: 0.0316 | train_micro_f1: 0.9842\n",
            "\tval_loss: 0.1080 | val_micro_f1: 0.9491\n",
            "epoch 00123\n",
            "\ttrain_loss: 0.0321 | train_micro_f1: 0.9834\n",
            "\tval_loss: 0.1096 | val_micro_f1: 0.9488\n",
            "epoch 00124\n",
            "\ttrain_loss: 0.0329 | train_micro_f1: 0.9833\n",
            "\tval_loss: 0.1100 | val_micro_f1: 0.9496\n",
            "epoch 00125\n",
            "\ttrain_loss: 0.0345 | train_micro_f1: 0.9823\n",
            "\tval_loss: 0.1084 | val_micro_f1: 0.9501\n",
            "epoch 00126\n",
            "\ttrain_loss: 0.0349 | train_micro_f1: 0.9814\n",
            "\tval_loss: 0.1141 | val_micro_f1: 0.9459\n",
            "epoch 00127\n",
            "\ttrain_loss: 0.0375 | train_micro_f1: 0.9795\n",
            "\tval_loss: 0.1184 | val_micro_f1: 0.9431\n",
            "epoch 00128\n",
            "\ttrain_loss: 0.0404 | train_micro_f1: 0.9775\n",
            "\tval_loss: 0.1163 | val_micro_f1: 0.9458\n",
            "epoch 00129\n",
            "\ttrain_loss: 0.0418 | train_micro_f1: 0.9766\n",
            "\tval_loss: 0.1170 | val_micro_f1: 0.9450\n",
            "epoch 00130\n",
            "\ttrain_loss: 0.0451 | train_micro_f1: 0.9747\n",
            "\tval_loss: 0.1202 | val_micro_f1: 0.9421\n",
            "epoch 00131\n",
            "\ttrain_loss: 0.0458 | train_micro_f1: 0.9743\n",
            "\tval_loss: 0.1188 | val_micro_f1: 0.9430\n",
            "epoch 00132\n",
            "\ttrain_loss: 0.0441 | train_micro_f1: 0.9751\n",
            "\tval_loss: 0.1202 | val_micro_f1: 0.9429\n",
            "epoch 00133\n",
            "\ttrain_loss: 0.0423 | train_micro_f1: 0.9762\n",
            "\tval_loss: 0.1162 | val_micro_f1: 0.9457\n",
            "epoch 00134\n",
            "\ttrain_loss: 0.0400 | train_micro_f1: 0.9776\n",
            "\tval_loss: 0.1181 | val_micro_f1: 0.9440\n",
            "epoch 00135\n",
            "\ttrain_loss: 0.0396 | train_micro_f1: 0.9779\n",
            "\tval_loss: 0.1191 | val_micro_f1: 0.9435\n",
            "epoch 00136\n",
            "\ttrain_loss: 0.0392 | train_micro_f1: 0.9785\n",
            "\tval_loss: 0.1176 | val_micro_f1: 0.9456\n",
            "epoch 00137\n",
            "\ttrain_loss: 0.0385 | train_micro_f1: 0.9791\n",
            "\tval_loss: 0.1144 | val_micro_f1: 0.9473\n",
            "epoch 00138\n",
            "\ttrain_loss: 0.0370 | train_micro_f1: 0.9799\n",
            "\tval_loss: 0.1101 | val_micro_f1: 0.9489\n",
            "epoch 00139\n",
            "\ttrain_loss: 0.0357 | train_micro_f1: 0.9805\n",
            "\tval_loss: 0.1136 | val_micro_f1: 0.9482\n",
            "epoch 00140\n",
            "\ttrain_loss: 0.0342 | train_micro_f1: 0.9814\n",
            "\tval_loss: 0.1135 | val_micro_f1: 0.9483\n",
            "epoch 00141\n",
            "\ttrain_loss: 0.0350 | train_micro_f1: 0.9811\n",
            "\tval_loss: 0.1136 | val_micro_f1: 0.9487\n",
            "epoch 00142\n",
            "\ttrain_loss: 0.0346 | train_micro_f1: 0.9816\n",
            "\tval_loss: 0.1190 | val_micro_f1: 0.9457\n",
            "epoch 00143\n",
            "\ttrain_loss: 0.0334 | train_micro_f1: 0.9824\n",
            "\tval_loss: 0.1159 | val_micro_f1: 0.9483\n",
            "epoch 00144\n",
            "\ttrain_loss: 0.0309 | train_micro_f1: 0.9840\n",
            "\tval_loss: 0.1127 | val_micro_f1: 0.9491\n",
            "epoch 00145\n",
            "\ttrain_loss: 0.0292 | train_micro_f1: 0.9845\n",
            "\tval_loss: 0.1115 | val_micro_f1: 0.9505\n",
            "epoch 00146\n",
            "\ttrain_loss: 0.0287 | train_micro_f1: 0.9851\n",
            "\tval_loss: 0.1102 | val_micro_f1: 0.9517\n",
            "epoch 00147\n",
            "\ttrain_loss: 0.0268 | train_micro_f1: 0.9862\n",
            "\tval_loss: 0.1079 | val_micro_f1: 0.9533\n",
            "epoch 00148\n",
            "\ttrain_loss: 0.0268 | train_micro_f1: 0.9864\n",
            "\tval_loss: 0.1082 | val_micro_f1: 0.9529\n",
            "epoch 00149\n",
            "\ttrain_loss: 0.0272 | train_micro_f1: 0.9866\n",
            "\tval_loss: 0.1090 | val_micro_f1: 0.9527\n",
            "epoch 00150\n",
            "\ttrain_loss: 0.0265 | train_micro_f1: 0.9868\n",
            "\tval_loss: 0.1095 | val_micro_f1: 0.9534\n",
            "epoch 00151\n",
            "\ttrain_loss: 0.0272 | train_micro_f1: 0.9864\n",
            "\tval_loss: 0.1107 | val_micro_f1: 0.9522\n",
            "epoch 00152\n",
            "\ttrain_loss: 0.0258 | train_micro_f1: 0.9870\n",
            "\tval_loss: 0.1103 | val_micro_f1: 0.9521\n",
            "epoch 00153\n",
            "\ttrain_loss: 0.0251 | train_micro_f1: 0.9876\n",
            "\tval_loss: 0.1096 | val_micro_f1: 0.9538\n",
            "epoch 00154\n",
            "\ttrain_loss: 0.0244 | train_micro_f1: 0.9878\n",
            "\tval_loss: 0.1093 | val_micro_f1: 0.9548\n",
            "epoch 00155\n",
            "\ttrain_loss: 0.0251 | train_micro_f1: 0.9876\n",
            "\tval_loss: 0.1085 | val_micro_f1: 0.9542\n",
            "epoch 00156\n",
            "\ttrain_loss: 0.0249 | train_micro_f1: 0.9877\n",
            "\tval_loss: 0.1092 | val_micro_f1: 0.9535\n",
            "epoch 00157\n",
            "\ttrain_loss: 0.0255 | train_micro_f1: 0.9871\n",
            "\tval_loss: 0.1112 | val_micro_f1: 0.9534\n",
            "epoch 00158\n",
            "\ttrain_loss: 0.0258 | train_micro_f1: 0.9868\n",
            "\tval_loss: 0.1092 | val_micro_f1: 0.9542\n",
            "epoch 00159\n",
            "\ttrain_loss: 0.0259 | train_micro_f1: 0.9869\n",
            "\tval_loss: 0.1108 | val_micro_f1: 0.9531\n",
            "epoch 00160\n",
            "\ttrain_loss: 0.0258 | train_micro_f1: 0.9864\n",
            "\tval_loss: 0.1107 | val_micro_f1: 0.9526\n",
            "epoch 00161\n",
            "\ttrain_loss: 0.0272 | train_micro_f1: 0.9854\n",
            "\tval_loss: 0.1134 | val_micro_f1: 0.9528\n",
            "epoch 00162\n",
            "\ttrain_loss: 0.0280 | train_micro_f1: 0.9854\n",
            "\tval_loss: 0.1145 | val_micro_f1: 0.9522\n",
            "epoch 00163\n",
            "\ttrain_loss: 0.0263 | train_micro_f1: 0.9866\n",
            "\tval_loss: 0.1139 | val_micro_f1: 0.9520\n",
            "epoch 00164\n",
            "\ttrain_loss: 0.0256 | train_micro_f1: 0.9867\n",
            "\tval_loss: 0.1142 | val_micro_f1: 0.9527\n",
            "epoch 00165\n",
            "\ttrain_loss: 0.0250 | train_micro_f1: 0.9870\n",
            "\tval_loss: 0.1150 | val_micro_f1: 0.9528\n",
            "epoch 00166\n",
            "\ttrain_loss: 0.0254 | train_micro_f1: 0.9868\n",
            "\tval_loss: 0.1152 | val_micro_f1: 0.9518\n",
            "epoch 00167\n",
            "\ttrain_loss: 0.0249 | train_micro_f1: 0.9873\n",
            "\tval_loss: 0.1113 | val_micro_f1: 0.9538\n",
            "epoch 00168\n",
            "\ttrain_loss: 0.0242 | train_micro_f1: 0.9877\n",
            "\tval_loss: 0.1108 | val_micro_f1: 0.9545\n",
            "epoch 00169\n",
            "\ttrain_loss: 0.0244 | train_micro_f1: 0.9874\n",
            "\tval_loss: 0.1132 | val_micro_f1: 0.9536\n",
            "epoch 00170\n",
            "\ttrain_loss: 0.0248 | train_micro_f1: 0.9872\n",
            "\tval_loss: 0.1118 | val_micro_f1: 0.9544\n",
            "epoch 00171\n",
            "\ttrain_loss: 0.0263 | train_micro_f1: 0.9864\n",
            "\tval_loss: 0.1142 | val_micro_f1: 0.9531\n",
            "epoch 00172\n",
            "\ttrain_loss: 0.0274 | train_micro_f1: 0.9857\n",
            "\tval_loss: 0.1102 | val_micro_f1: 0.9545\n",
            "epoch 00173\n",
            "\ttrain_loss: 0.0313 | train_micro_f1: 0.9827\n",
            "\tval_loss: 0.1152 | val_micro_f1: 0.9522\n",
            "epoch 00174\n",
            "\ttrain_loss: 0.0347 | train_micro_f1: 0.9807\n",
            "\tval_loss: 0.1288 | val_micro_f1: 0.9443\n",
            "epoch 00175\n",
            "\ttrain_loss: 0.0454 | train_micro_f1: 0.9738\n",
            "\tval_loss: 0.1389 | val_micro_f1: 0.9372\n",
            "epoch 00176\n",
            "\ttrain_loss: 0.0502 | train_micro_f1: 0.9711\n",
            "\tval_loss: 0.1378 | val_micro_f1: 0.9392\n",
            "epoch 00177\n",
            "\ttrain_loss: 0.0585 | train_micro_f1: 0.9662\n",
            "\tval_loss: 0.1411 | val_micro_f1: 0.9362\n",
            "epoch 00178\n",
            "\ttrain_loss: 0.0596 | train_micro_f1: 0.9656\n",
            "\tval_loss: 0.1512 | val_micro_f1: 0.9320\n",
            "epoch 00179\n",
            "\ttrain_loss: 0.0670 | train_micro_f1: 0.9618\n",
            "\tval_loss: 0.1528 | val_micro_f1: 0.9301\n",
            "epoch 00180\n",
            "\ttrain_loss: 0.0742 | train_micro_f1: 0.9579\n",
            "\tval_loss: 0.1483 | val_micro_f1: 0.9331\n",
            "epoch 00181\n",
            "\ttrain_loss: 0.0681 | train_micro_f1: 0.9616\n",
            "\tval_loss: 0.1458 | val_micro_f1: 0.9330\n",
            "epoch 00182\n",
            "\ttrain_loss: 0.0583 | train_micro_f1: 0.9665\n",
            "\tval_loss: 0.1348 | val_micro_f1: 0.9401\n",
            "epoch 00183\n",
            "\ttrain_loss: 0.0488 | train_micro_f1: 0.9721\n",
            "\tval_loss: 0.1262 | val_micro_f1: 0.9446\n",
            "epoch 00184\n",
            "\ttrain_loss: 0.0398 | train_micro_f1: 0.9776\n",
            "\tval_loss: 0.1198 | val_micro_f1: 0.9487\n",
            "epoch 00185\n",
            "\ttrain_loss: 0.0333 | train_micro_f1: 0.9818\n",
            "\tval_loss: 0.1141 | val_micro_f1: 0.9520\n",
            "epoch 00186\n",
            "\ttrain_loss: 0.0290 | train_micro_f1: 0.9849\n",
            "\tval_loss: 0.1112 | val_micro_f1: 0.9539\n",
            "epoch 00187\n",
            "\ttrain_loss: 0.0244 | train_micro_f1: 0.9872\n",
            "\tval_loss: 0.1066 | val_micro_f1: 0.9562\n",
            "epoch 00188\n",
            "\ttrain_loss: 0.0222 | train_micro_f1: 0.9888\n",
            "\tval_loss: 0.1047 | val_micro_f1: 0.9577\n",
            "epoch 00189\n",
            "\ttrain_loss: 0.0205 | train_micro_f1: 0.9898\n",
            "\tval_loss: 0.1078 | val_micro_f1: 0.9570\n",
            "epoch 00190\n",
            "\ttrain_loss: 0.0199 | train_micro_f1: 0.9907\n",
            "\tval_loss: 0.1059 | val_micro_f1: 0.9582\n",
            "epoch 00191\n",
            "\ttrain_loss: 0.0175 | train_micro_f1: 0.9917\n",
            "\tval_loss: 0.1050 | val_micro_f1: 0.9589\n",
            "epoch 00192\n",
            "\ttrain_loss: 0.0171 | train_micro_f1: 0.9919\n",
            "\tval_loss: 0.1049 | val_micro_f1: 0.9598\n",
            "epoch 00193\n",
            "\ttrain_loss: 0.0173 | train_micro_f1: 0.9920\n",
            "\tval_loss: 0.1064 | val_micro_f1: 0.9590\n",
            "epoch 00194\n",
            "\ttrain_loss: 0.0169 | train_micro_f1: 0.9926\n",
            "\tval_loss: 0.1043 | val_micro_f1: 0.9592\n",
            "epoch 00195\n",
            "\ttrain_loss: 0.0153 | train_micro_f1: 0.9931\n",
            "\tval_loss: 0.1040 | val_micro_f1: 0.9603\n",
            "epoch 00196\n",
            "\ttrain_loss: 0.0157 | train_micro_f1: 0.9930\n",
            "\tval_loss: 0.1040 | val_micro_f1: 0.9599\n",
            "epoch 00197\n",
            "\ttrain_loss: 0.0157 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1052 | val_micro_f1: 0.9604\n",
            "epoch 00198\n",
            "\ttrain_loss: 0.0164 | train_micro_f1: 0.9928\n",
            "\tval_loss: 0.1051 | val_micro_f1: 0.9595\n",
            "epoch 00199\n",
            "\ttrain_loss: 0.0160 | train_micro_f1: 0.9927\n",
            "\tval_loss: 0.1085 | val_micro_f1: 0.9587\n",
            "epoch 00200\n",
            "\ttrain_loss: 0.0161 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1071 | val_micro_f1: 0.9588\n",
            "epoch 00201\n",
            "\ttrain_loss: 0.0166 | train_micro_f1: 0.9919\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9581\n",
            "epoch 00202\n",
            "\ttrain_loss: 0.0179 | train_micro_f1: 0.9925\n",
            "\tval_loss: 0.1140 | val_micro_f1: 0.9563\n",
            "epoch 00203\n",
            "\ttrain_loss: 0.0165 | train_micro_f1: 0.9922\n",
            "\tval_loss: 0.1128 | val_micro_f1: 0.9569\n",
            "epoch 00204\n",
            "\ttrain_loss: 0.0164 | train_micro_f1: 0.9924\n",
            "\tval_loss: 0.1059 | val_micro_f1: 0.9599\n",
            "epoch 00205\n",
            "\ttrain_loss: 0.0179 | train_micro_f1: 0.9913\n",
            "\tval_loss: 0.1090 | val_micro_f1: 0.9589\n",
            "epoch 00206\n",
            "\ttrain_loss: 0.0199 | train_micro_f1: 0.9909\n",
            "\tval_loss: 0.1122 | val_micro_f1: 0.9565\n",
            "epoch 00207\n",
            "\ttrain_loss: 0.0193 | train_micro_f1: 0.9905\n",
            "\tval_loss: 0.1125 | val_micro_f1: 0.9567\n",
            "epoch 00208\n",
            "\ttrain_loss: 0.0191 | train_micro_f1: 0.9907\n",
            "\tval_loss: 0.1130 | val_micro_f1: 0.9570\n",
            "epoch 00209\n",
            "\ttrain_loss: 0.0189 | train_micro_f1: 0.9906\n",
            "\tval_loss: 0.1147 | val_micro_f1: 0.9564\n",
            "epoch 00210\n",
            "\ttrain_loss: 0.0203 | train_micro_f1: 0.9902\n",
            "\tval_loss: 0.1138 | val_micro_f1: 0.9565\n",
            "epoch 00211\n",
            "\ttrain_loss: 0.0204 | train_micro_f1: 0.9901\n",
            "\tval_loss: 0.1139 | val_micro_f1: 0.9564\n",
            "epoch 00212\n",
            "\ttrain_loss: 0.0188 | train_micro_f1: 0.9909\n",
            "\tval_loss: 0.1126 | val_micro_f1: 0.9572\n",
            "epoch 00213\n",
            "\ttrain_loss: 0.0176 | train_micro_f1: 0.9913\n",
            "\tval_loss: 0.1122 | val_micro_f1: 0.9577\n",
            "epoch 00214\n",
            "\ttrain_loss: 0.0174 | train_micro_f1: 0.9917\n",
            "\tval_loss: 0.1120 | val_micro_f1: 0.9579\n",
            "epoch 00215\n",
            "\ttrain_loss: 0.0166 | train_micro_f1: 0.9919\n",
            "\tval_loss: 0.1122 | val_micro_f1: 0.9581\n",
            "epoch 00216\n",
            "\ttrain_loss: 0.0175 | train_micro_f1: 0.9915\n",
            "\tval_loss: 0.1096 | val_micro_f1: 0.9589\n",
            "epoch 00217\n",
            "\ttrain_loss: 0.0173 | train_micro_f1: 0.9916\n",
            "\tval_loss: 0.1157 | val_micro_f1: 0.9573\n",
            "epoch 00218\n",
            "\ttrain_loss: 0.0175 | train_micro_f1: 0.9918\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9589\n",
            "epoch 00219\n",
            "\ttrain_loss: 0.0174 | train_micro_f1: 0.9915\n",
            "\tval_loss: 0.1155 | val_micro_f1: 0.9575\n",
            "epoch 00220\n",
            "\ttrain_loss: 0.0172 | train_micro_f1: 0.9920\n",
            "\tval_loss: 0.1120 | val_micro_f1: 0.9586\n",
            "epoch 00221\n",
            "\ttrain_loss: 0.0170 | train_micro_f1: 0.9918\n",
            "\tval_loss: 0.1130 | val_micro_f1: 0.9587\n",
            "epoch 00222\n",
            "\ttrain_loss: 0.0178 | train_micro_f1: 0.9919\n",
            "\tval_loss: 0.1125 | val_micro_f1: 0.9586\n",
            "epoch 00223\n",
            "\ttrain_loss: 0.0186 | train_micro_f1: 0.9909\n",
            "\tval_loss: 0.1159 | val_micro_f1: 0.9567\n",
            "epoch 00224\n",
            "\ttrain_loss: 0.0183 | train_micro_f1: 0.9912\n",
            "\tval_loss: 0.1111 | val_micro_f1: 0.9586\n",
            "epoch 00225\n",
            "\ttrain_loss: 0.0184 | train_micro_f1: 0.9908\n",
            "\tval_loss: 0.1171 | val_micro_f1: 0.9568\n",
            "epoch 00226\n",
            "\ttrain_loss: 0.0190 | train_micro_f1: 0.9908\n",
            "\tval_loss: 0.1159 | val_micro_f1: 0.9568\n",
            "epoch 00227\n",
            "\ttrain_loss: 0.0189 | train_micro_f1: 0.9903\n",
            "\tval_loss: 0.1192 | val_micro_f1: 0.9566\n",
            "epoch 00228\n",
            "\ttrain_loss: 0.0196 | train_micro_f1: 0.9907\n",
            "\tval_loss: 0.1166 | val_micro_f1: 0.9577\n",
            "epoch 00229\n",
            "\ttrain_loss: 0.0183 | train_micro_f1: 0.9907\n",
            "\tval_loss: 0.1186 | val_micro_f1: 0.9575\n",
            "epoch 00230\n",
            "\ttrain_loss: 0.0189 | train_micro_f1: 0.9908\n",
            "\tval_loss: 0.1180 | val_micro_f1: 0.9562\n",
            "epoch 00231\n",
            "\ttrain_loss: 0.0177 | train_micro_f1: 0.9910\n",
            "\tval_loss: 0.1137 | val_micro_f1: 0.9583\n",
            "epoch 00232\n",
            "\ttrain_loss: 0.0180 | train_micro_f1: 0.9911\n",
            "\tval_loss: 0.1131 | val_micro_f1: 0.9581\n",
            "epoch 00233\n",
            "\ttrain_loss: 0.0177 | train_micro_f1: 0.9909\n",
            "\tval_loss: 0.1146 | val_micro_f1: 0.9580\n",
            "epoch 00234\n",
            "\ttrain_loss: 0.0187 | train_micro_f1: 0.9908\n",
            "\tval_loss: 0.1159 | val_micro_f1: 0.9567\n",
            "epoch 00235\n",
            "\ttrain_loss: 0.0195 | train_micro_f1: 0.9898\n",
            "\tval_loss: 0.1221 | val_micro_f1: 0.9558\n",
            "epoch 00236\n",
            "\ttrain_loss: 0.0204 | train_micro_f1: 0.9896\n",
            "\tval_loss: 0.1182 | val_micro_f1: 0.9560\n",
            "epoch 00237\n",
            "\ttrain_loss: 0.0204 | train_micro_f1: 0.9893\n",
            "\tval_loss: 0.1200 | val_micro_f1: 0.9567\n",
            "epoch 00238\n",
            "\ttrain_loss: 0.0211 | train_micro_f1: 0.9895\n",
            "\tval_loss: 0.1180 | val_micro_f1: 0.9565\n",
            "epoch 00239\n",
            "\ttrain_loss: 0.0206 | train_micro_f1: 0.9889\n",
            "\tval_loss: 0.1161 | val_micro_f1: 0.9580\n",
            "epoch 00240\n",
            "\ttrain_loss: 0.0212 | train_micro_f1: 0.9894\n",
            "\tval_loss: 0.1143 | val_micro_f1: 0.9576\n",
            "epoch 00241\n",
            "\ttrain_loss: 0.0215 | train_micro_f1: 0.9885\n",
            "\tval_loss: 0.1145 | val_micro_f1: 0.9579\n",
            "epoch 00242\n",
            "\ttrain_loss: 0.0234 | train_micro_f1: 0.9879\n",
            "\tval_loss: 0.1201 | val_micro_f1: 0.9551\n",
            "epoch 00243\n",
            "\ttrain_loss: 0.0237 | train_micro_f1: 0.9876\n",
            "\tval_loss: 0.1240 | val_micro_f1: 0.9547\n",
            "epoch 00244\n",
            "\ttrain_loss: 0.0354 | train_micro_f1: 0.9816\n",
            "\tval_loss: 0.1485 | val_micro_f1: 0.9422\n",
            "epoch 00245\n",
            "\ttrain_loss: 0.0747 | train_micro_f1: 0.9630\n",
            "\tval_loss: 0.1720 | val_micro_f1: 0.9286\n",
            "epoch 00246\n",
            "\ttrain_loss: 0.0900 | train_micro_f1: 0.9543\n",
            "\tval_loss: 0.1649 | val_micro_f1: 0.9299\n",
            "epoch 00247\n",
            "\ttrain_loss: 0.0679 | train_micro_f1: 0.9625\n",
            "\tval_loss: 0.1512 | val_micro_f1: 0.9377\n",
            "epoch 00248\n",
            "\ttrain_loss: 0.0511 | train_micro_f1: 0.9715\n",
            "\tval_loss: 0.1360 | val_micro_f1: 0.9457\n",
            "epoch 00249\n",
            "\ttrain_loss: 0.0401 | train_micro_f1: 0.9775\n",
            "\tval_loss: 0.1290 | val_micro_f1: 0.9477\n",
            "epoch 00250\n",
            "\ttrain_loss: 0.0379 | train_micro_f1: 0.9787\n",
            "\tval_loss: 0.1207 | val_micro_f1: 0.9537\n",
            "epoch 00251\n",
            "\ttrain_loss: 0.0523 | train_micro_f1: 0.9702\n",
            "\tval_loss: 0.1576 | val_micro_f1: 0.9339\n",
            "epoch 00252\n",
            "\ttrain_loss: 0.0541 | train_micro_f1: 0.9693\n",
            "\tval_loss: 0.1478 | val_micro_f1: 0.9398\n",
            "epoch 00253\n",
            "\ttrain_loss: 0.0483 | train_micro_f1: 0.9734\n",
            "\tval_loss: 0.1309 | val_micro_f1: 0.9478\n",
            "epoch 00254\n",
            "\ttrain_loss: 0.0387 | train_micro_f1: 0.9782\n",
            "\tval_loss: 0.1269 | val_micro_f1: 0.9504\n",
            "epoch 00255\n",
            "\ttrain_loss: 0.0310 | train_micro_f1: 0.9833\n",
            "\tval_loss: 0.1162 | val_micro_f1: 0.9548\n",
            "epoch 00256\n",
            "\ttrain_loss: 0.0242 | train_micro_f1: 0.9868\n",
            "\tval_loss: 0.1174 | val_micro_f1: 0.9568\n",
            "epoch 00257\n",
            "\ttrain_loss: 0.0201 | train_micro_f1: 0.9894\n",
            "\tval_loss: 0.1137 | val_micro_f1: 0.9594\n",
            "epoch 00258\n",
            "\ttrain_loss: 0.0169 | train_micro_f1: 0.9915\n",
            "\tval_loss: 0.1073 | val_micro_f1: 0.9622\n",
            "epoch 00259\n",
            "\ttrain_loss: 0.0151 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1042 | val_micro_f1: 0.9634\n",
            "epoch 00260\n",
            "\ttrain_loss: 0.0136 | train_micro_f1: 0.9935\n",
            "\tval_loss: 0.1045 | val_micro_f1: 0.9640\n",
            "epoch 00261\n",
            "\ttrain_loss: 0.0138 | train_micro_f1: 0.9938\n",
            "\tval_loss: 0.1067 | val_micro_f1: 0.9633\n",
            "epoch 00262\n",
            "\ttrain_loss: 0.0132 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1071 | val_micro_f1: 0.9634\n",
            "epoch 00263\n",
            "\ttrain_loss: 0.0134 | train_micro_f1: 0.9942\n",
            "\tval_loss: 0.1077 | val_micro_f1: 0.9639\n",
            "epoch 00264\n",
            "\ttrain_loss: 0.0130 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1091 | val_micro_f1: 0.9633\n",
            "epoch 00265\n",
            "\ttrain_loss: 0.0137 | train_micro_f1: 0.9941\n",
            "\tval_loss: 0.1083 | val_micro_f1: 0.9634\n",
            "epoch 00266\n",
            "\ttrain_loss: 0.0131 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1075 | val_micro_f1: 0.9640\n",
            "epoch 00267\n",
            "\ttrain_loss: 0.0141 | train_micro_f1: 0.9937\n",
            "\tval_loss: 0.1092 | val_micro_f1: 0.9633\n",
            "epoch 00268\n",
            "\ttrain_loss: 0.0150 | train_micro_f1: 0.9928\n",
            "\tval_loss: 0.1114 | val_micro_f1: 0.9620\n",
            "epoch 00269\n",
            "\ttrain_loss: 0.0150 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1138 | val_micro_f1: 0.9612\n",
            "epoch 00270\n",
            "\ttrain_loss: 0.0139 | train_micro_f1: 0.9932\n",
            "\tval_loss: 0.1111 | val_micro_f1: 0.9622\n",
            "epoch 00271\n",
            "\ttrain_loss: 0.0143 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1149 | val_micro_f1: 0.9606\n",
            "epoch 00272\n",
            "\ttrain_loss: 0.0148 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1140 | val_micro_f1: 0.9610\n",
            "epoch 00273\n",
            "\ttrain_loss: 0.0153 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1152 | val_micro_f1: 0.9606\n",
            "epoch 00274\n",
            "\ttrain_loss: 0.0143 | train_micro_f1: 0.9932\n",
            "\tval_loss: 0.1142 | val_micro_f1: 0.9615\n",
            "epoch 00275\n",
            "\ttrain_loss: 0.0138 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1102 | val_micro_f1: 0.9628\n",
            "epoch 00276\n",
            "\ttrain_loss: 0.0135 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1109 | val_micro_f1: 0.9628\n",
            "epoch 00277\n",
            "\ttrain_loss: 0.0140 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1123 | val_micro_f1: 0.9620\n",
            "epoch 00278\n",
            "\ttrain_loss: 0.0138 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1156 | val_micro_f1: 0.9613\n",
            "epoch 00279\n",
            "\ttrain_loss: 0.0147 | train_micro_f1: 0.9932\n",
            "\tval_loss: 0.1165 | val_micro_f1: 0.9610\n",
            "epoch 00280\n",
            "\ttrain_loss: 0.0139 | train_micro_f1: 0.9932\n",
            "\tval_loss: 0.1135 | val_micro_f1: 0.9625\n",
            "epoch 00281\n",
            "\ttrain_loss: 0.0150 | train_micro_f1: 0.9931\n",
            "\tval_loss: 0.1146 | val_micro_f1: 0.9623\n",
            "epoch 00282\n",
            "\ttrain_loss: 0.0144 | train_micro_f1: 0.9930\n",
            "\tval_loss: 0.1131 | val_micro_f1: 0.9632\n",
            "epoch 00283\n",
            "\ttrain_loss: 0.0155 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1135 | val_micro_f1: 0.9616\n",
            "epoch 00284\n",
            "\ttrain_loss: 0.0142 | train_micro_f1: 0.9931\n",
            "\tval_loss: 0.1134 | val_micro_f1: 0.9628\n",
            "epoch 00285\n",
            "\ttrain_loss: 0.0150 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1112 | val_micro_f1: 0.9627\n",
            "epoch 00286\n",
            "\ttrain_loss: 0.0133 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1135 | val_micro_f1: 0.9627\n",
            "epoch 00287\n",
            "\ttrain_loss: 0.0143 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1123 | val_micro_f1: 0.9631\n",
            "epoch 00288\n",
            "\ttrain_loss: 0.0150 | train_micro_f1: 0.9926\n",
            "\tval_loss: 0.1179 | val_micro_f1: 0.9608\n",
            "epoch 00289\n",
            "\ttrain_loss: 0.0155 | train_micro_f1: 0.9926\n",
            "\tval_loss: 0.1181 | val_micro_f1: 0.9603\n",
            "epoch 00290\n",
            "\ttrain_loss: 0.0145 | train_micro_f1: 0.9930\n",
            "\tval_loss: 0.1176 | val_micro_f1: 0.9598\n",
            "epoch 00291\n",
            "\ttrain_loss: 0.0143 | train_micro_f1: 0.9928\n",
            "\tval_loss: 0.1148 | val_micro_f1: 0.9618\n",
            "epoch 00292\n",
            "\ttrain_loss: 0.0143 | train_micro_f1: 0.9930\n",
            "\tval_loss: 0.1187 | val_micro_f1: 0.9602\n",
            "epoch 00293\n",
            "\ttrain_loss: 0.0142 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1168 | val_micro_f1: 0.9609\n",
            "epoch 00294\n",
            "\ttrain_loss: 0.0150 | train_micro_f1: 0.9927\n",
            "\tval_loss: 0.1224 | val_micro_f1: 0.9595\n",
            "epoch 00295\n",
            "\ttrain_loss: 0.0145 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1217 | val_micro_f1: 0.9594\n",
            "epoch 00296\n",
            "\ttrain_loss: 0.0140 | train_micro_f1: 0.9931\n",
            "\tval_loss: 0.1174 | val_micro_f1: 0.9606\n",
            "epoch 00297\n",
            "\ttrain_loss: 0.0140 | train_micro_f1: 0.9931\n",
            "\tval_loss: 0.1106 | val_micro_f1: 0.9632\n",
            "epoch 00298\n",
            "\ttrain_loss: 0.0134 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1114 | val_micro_f1: 0.9626\n",
            "epoch 00299\n",
            "\ttrain_loss: 0.0132 | train_micro_f1: 0.9935\n",
            "\tval_loss: 0.1136 | val_micro_f1: 0.9631\n",
            "epoch 00300\n",
            "\ttrain_loss: 0.0134 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1163 | val_micro_f1: 0.9621\n",
            "epoch 00301\n",
            "\ttrain_loss: 0.0141 | train_micro_f1: 0.9932\n",
            "\tval_loss: 0.1156 | val_micro_f1: 0.9627\n",
            "epoch 00302\n",
            "\ttrain_loss: 0.0134 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1158 | val_micro_f1: 0.9626\n",
            "epoch 00303\n",
            "\ttrain_loss: 0.0133 | train_micro_f1: 0.9937\n",
            "\tval_loss: 0.1146 | val_micro_f1: 0.9631\n",
            "epoch 00304\n",
            "\ttrain_loss: 0.0127 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1175 | val_micro_f1: 0.9617\n",
            "epoch 00305\n",
            "\ttrain_loss: 0.0129 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1176 | val_micro_f1: 0.9620\n",
            "epoch 00306\n",
            "\ttrain_loss: 0.0131 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1167 | val_micro_f1: 0.9630\n",
            "epoch 00307\n",
            "\ttrain_loss: 0.0129 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1173 | val_micro_f1: 0.9627\n",
            "epoch 00308\n",
            "\ttrain_loss: 0.0137 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1213 | val_micro_f1: 0.9611\n",
            "epoch 00309\n",
            "\ttrain_loss: 0.0123 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1166 | val_micro_f1: 0.9627\n",
            "epoch 00310\n",
            "\ttrain_loss: 0.0121 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1145 | val_micro_f1: 0.9630\n",
            "epoch 00311\n",
            "\ttrain_loss: 0.0117 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1119 | val_micro_f1: 0.9642\n",
            "epoch 00312\n",
            "\ttrain_loss: 0.0112 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1108 | val_micro_f1: 0.9647\n",
            "epoch 00313\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9942\n",
            "\tval_loss: 0.1147 | val_micro_f1: 0.9641\n",
            "epoch 00314\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1167 | val_micro_f1: 0.9627\n",
            "epoch 00315\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1175 | val_micro_f1: 0.9627\n",
            "epoch 00316\n",
            "\ttrain_loss: 0.0117 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1146 | val_micro_f1: 0.9640\n",
            "epoch 00317\n",
            "\ttrain_loss: 0.0117 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1128 | val_micro_f1: 0.9648\n",
            "epoch 00318\n",
            "\ttrain_loss: 0.0126 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1130 | val_micro_f1: 0.9649\n",
            "epoch 00319\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9941\n",
            "\tval_loss: 0.1189 | val_micro_f1: 0.9631\n",
            "epoch 00320\n",
            "\ttrain_loss: 0.0128 | train_micro_f1: 0.9938\n",
            "\tval_loss: 0.1213 | val_micro_f1: 0.9623\n",
            "epoch 00321\n",
            "\ttrain_loss: 0.0126 | train_micro_f1: 0.9941\n",
            "\tval_loss: 0.1218 | val_micro_f1: 0.9618\n",
            "epoch 00322\n",
            "\ttrain_loss: 0.0129 | train_micro_f1: 0.9935\n",
            "\tval_loss: 0.1211 | val_micro_f1: 0.9615\n",
            "epoch 00323\n",
            "\ttrain_loss: 0.0147 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1213 | val_micro_f1: 0.9604\n",
            "epoch 00324\n",
            "\ttrain_loss: 0.0153 | train_micro_f1: 0.9926\n",
            "\tval_loss: 0.1238 | val_micro_f1: 0.9602\n",
            "epoch 00325\n",
            "\ttrain_loss: 0.0149 | train_micro_f1: 0.9927\n",
            "\tval_loss: 0.1238 | val_micro_f1: 0.9594\n",
            "epoch 00326\n",
            "\ttrain_loss: 0.0155 | train_micro_f1: 0.9919\n",
            "\tval_loss: 0.1270 | val_micro_f1: 0.9593\n",
            "epoch 00327\n",
            "\ttrain_loss: 0.0164 | train_micro_f1: 0.9926\n",
            "\tval_loss: 0.1247 | val_micro_f1: 0.9606\n",
            "epoch 00328\n",
            "\ttrain_loss: 0.0141 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1219 | val_micro_f1: 0.9608\n",
            "epoch 00329\n",
            "\ttrain_loss: 0.0138 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1195 | val_micro_f1: 0.9617\n",
            "epoch 00330\n",
            "\ttrain_loss: 0.0130 | train_micro_f1: 0.9932\n",
            "\tval_loss: 0.1216 | val_micro_f1: 0.9619\n",
            "epoch 00331\n",
            "\ttrain_loss: 0.0130 | train_micro_f1: 0.9942\n",
            "\tval_loss: 0.1165 | val_micro_f1: 0.9635\n",
            "epoch 00332\n",
            "\ttrain_loss: 0.0128 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1184 | val_micro_f1: 0.9624\n",
            "epoch 00333\n",
            "\ttrain_loss: 0.0132 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1193 | val_micro_f1: 0.9624\n",
            "epoch 00334\n",
            "\ttrain_loss: 0.0123 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1205 | val_micro_f1: 0.9627\n",
            "epoch 00335\n",
            "\ttrain_loss: 0.0124 | train_micro_f1: 0.9946\n",
            "\tval_loss: 0.1159 | val_micro_f1: 0.9647\n",
            "epoch 00336\n",
            "\ttrain_loss: 0.0119 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1162 | val_micro_f1: 0.9644\n",
            "epoch 00337\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1173 | val_micro_f1: 0.9641\n",
            "epoch 00338\n",
            "\ttrain_loss: 0.0123 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1201 | val_micro_f1: 0.9623\n",
            "epoch 00339\n",
            "\ttrain_loss: 0.0136 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1236 | val_micro_f1: 0.9610\n",
            "epoch 00340\n",
            "\ttrain_loss: 0.0129 | train_micro_f1: 0.9935\n",
            "\tval_loss: 0.1239 | val_micro_f1: 0.9607\n",
            "epoch 00341\n",
            "\ttrain_loss: 0.0143 | train_micro_f1: 0.9929\n",
            "\tval_loss: 0.1242 | val_micro_f1: 0.9609\n",
            "epoch 00342\n",
            "\ttrain_loss: 0.0156 | train_micro_f1: 0.9921\n",
            "\tval_loss: 0.1226 | val_micro_f1: 0.9604\n",
            "epoch 00343\n",
            "\ttrain_loss: 0.0155 | train_micro_f1: 0.9921\n",
            "\tval_loss: 0.1225 | val_micro_f1: 0.9611\n",
            "epoch 00344\n",
            "\ttrain_loss: 0.0175 | train_micro_f1: 0.9915\n",
            "\tval_loss: 0.1236 | val_micro_f1: 0.9605\n",
            "epoch 00345\n",
            "\ttrain_loss: 0.0184 | train_micro_f1: 0.9912\n",
            "\tval_loss: 0.1253 | val_micro_f1: 0.9599\n",
            "epoch 00346\n",
            "\ttrain_loss: 0.0170 | train_micro_f1: 0.9912\n",
            "\tval_loss: 0.1231 | val_micro_f1: 0.9611\n",
            "epoch 00347\n",
            "\ttrain_loss: 0.0167 | train_micro_f1: 0.9914\n",
            "\tval_loss: 0.1235 | val_micro_f1: 0.9611\n",
            "epoch 00348\n",
            "\ttrain_loss: 0.0158 | train_micro_f1: 0.9920\n",
            "\tval_loss: 0.1201 | val_micro_f1: 0.9621\n",
            "epoch 00349\n",
            "\ttrain_loss: 0.0142 | train_micro_f1: 0.9925\n",
            "\tval_loss: 0.1201 | val_micro_f1: 0.9621\n",
            "epoch 00350\n",
            "\ttrain_loss: 0.0136 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1188 | val_micro_f1: 0.9630\n",
            "epoch 00351\n",
            "\ttrain_loss: 0.0128 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1230 | val_micro_f1: 0.9617\n",
            "epoch 00352\n",
            "\ttrain_loss: 0.0127 | train_micro_f1: 0.9938\n",
            "\tval_loss: 0.1193 | val_micro_f1: 0.9632\n",
            "epoch 00353\n",
            "\ttrain_loss: 0.0122 | train_micro_f1: 0.9942\n",
            "\tval_loss: 0.1220 | val_micro_f1: 0.9635\n",
            "epoch 00354\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1160 | val_micro_f1: 0.9647\n",
            "epoch 00355\n",
            "\ttrain_loss: 0.0124 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1180 | val_micro_f1: 0.9648\n",
            "epoch 00356\n",
            "\ttrain_loss: 0.0119 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1171 | val_micro_f1: 0.9650\n",
            "epoch 00357\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1183 | val_micro_f1: 0.9649\n",
            "epoch 00358\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1131 | val_micro_f1: 0.9663\n",
            "epoch 00359\n",
            "\ttrain_loss: 0.0122 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1189 | val_micro_f1: 0.9649\n",
            "epoch 00360\n",
            "\ttrain_loss: 0.0126 | train_micro_f1: 0.9949\n",
            "\tval_loss: 0.1171 | val_micro_f1: 0.9650\n",
            "epoch 00361\n",
            "\ttrain_loss: 0.0119 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1217 | val_micro_f1: 0.9644\n",
            "epoch 00362\n",
            "\ttrain_loss: 0.0131 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1210 | val_micro_f1: 0.9634\n",
            "epoch 00363\n",
            "\ttrain_loss: 0.0109 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1179 | val_micro_f1: 0.9649\n",
            "epoch 00364\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9949\n",
            "\tval_loss: 0.1248 | val_micro_f1: 0.9625\n",
            "epoch 00365\n",
            "\ttrain_loss: 0.0103 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1205 | val_micro_f1: 0.9645\n",
            "epoch 00366\n",
            "\ttrain_loss: 0.0113 | train_micro_f1: 0.9949\n",
            "\tval_loss: 0.1240 | val_micro_f1: 0.9637\n",
            "epoch 00367\n",
            "\ttrain_loss: 0.0121 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1207 | val_micro_f1: 0.9648\n",
            "epoch 00368\n",
            "\ttrain_loss: 0.0140 | train_micro_f1: 0.9942\n",
            "\tval_loss: 0.1254 | val_micro_f1: 0.9628\n",
            "epoch 00369\n",
            "\ttrain_loss: 0.0122 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1219 | val_micro_f1: 0.9631\n",
            "epoch 00370\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1262 | val_micro_f1: 0.9625\n",
            "epoch 00371\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9941\n",
            "\tval_loss: 0.1224 | val_micro_f1: 0.9637\n",
            "epoch 00372\n",
            "\ttrain_loss: 0.0128 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1260 | val_micro_f1: 0.9633\n",
            "epoch 00373\n",
            "\ttrain_loss: 0.0128 | train_micro_f1: 0.9937\n",
            "\tval_loss: 0.1215 | val_micro_f1: 0.9629\n",
            "epoch 00374\n",
            "\ttrain_loss: 0.0141 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1218 | val_micro_f1: 0.9633\n",
            "epoch 00375\n",
            "\ttrain_loss: 0.0131 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1233 | val_micro_f1: 0.9635\n",
            "epoch 00376\n",
            "\ttrain_loss: 0.0123 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1226 | val_micro_f1: 0.9638\n",
            "epoch 00377\n",
            "\ttrain_loss: 0.0111 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1216 | val_micro_f1: 0.9646\n",
            "epoch 00378\n",
            "\ttrain_loss: 0.0114 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1212 | val_micro_f1: 0.9645\n",
            "epoch 00379\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1196 | val_micro_f1: 0.9647\n",
            "epoch 00380\n",
            "\ttrain_loss: 0.0117 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1200 | val_micro_f1: 0.9647\n",
            "epoch 00381\n",
            "\ttrain_loss: 0.0109 | train_micro_f1: 0.9950\n",
            "\tval_loss: 0.1174 | val_micro_f1: 0.9653\n",
            "epoch 00382\n",
            "\ttrain_loss: 0.0106 | train_micro_f1: 0.9949\n",
            "\tval_loss: 0.1203 | val_micro_f1: 0.9658\n",
            "epoch 00383\n",
            "\ttrain_loss: 0.0114 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1203 | val_micro_f1: 0.9647\n",
            "epoch 00384\n",
            "\ttrain_loss: 0.0121 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1235 | val_micro_f1: 0.9645\n",
            "epoch 00385\n",
            "\ttrain_loss: 0.0123 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1224 | val_micro_f1: 0.9654\n",
            "epoch 00386\n",
            "\ttrain_loss: 0.0113 | train_micro_f1: 0.9946\n",
            "\tval_loss: 0.1223 | val_micro_f1: 0.9643\n",
            "epoch 00387\n",
            "\ttrain_loss: 0.0115 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1214 | val_micro_f1: 0.9645\n",
            "epoch 00388\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9946\n",
            "\tval_loss: 0.1225 | val_micro_f1: 0.9643\n",
            "epoch 00389\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9946\n",
            "\tval_loss: 0.1243 | val_micro_f1: 0.9628\n",
            "epoch 00390\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1231 | val_micro_f1: 0.9641\n",
            "epoch 00391\n",
            "\ttrain_loss: 0.0111 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1206 | val_micro_f1: 0.9651\n",
            "epoch 00392\n",
            "\ttrain_loss: 0.0113 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1227 | val_micro_f1: 0.9641\n",
            "epoch 00393\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9946\n",
            "\tval_loss: 0.1213 | val_micro_f1: 0.9643\n",
            "epoch 00394\n",
            "\ttrain_loss: 0.0110 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1232 | val_micro_f1: 0.9651\n",
            "epoch 00395\n",
            "\ttrain_loss: 0.0106 | train_micro_f1: 0.9952\n",
            "\tval_loss: 0.1226 | val_micro_f1: 0.9646\n",
            "epoch 00396\n",
            "\ttrain_loss: 0.0107 | train_micro_f1: 0.9949\n",
            "\tval_loss: 0.1250 | val_micro_f1: 0.9647\n",
            "epoch 00397\n",
            "\ttrain_loss: 0.0113 | train_micro_f1: 0.9951\n",
            "\tval_loss: 0.1242 | val_micro_f1: 0.9648\n",
            "epoch 00398\n",
            "\ttrain_loss: 0.0115 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1208 | val_micro_f1: 0.9657\n",
            "epoch 00399\n",
            "\ttrain_loss: 0.0117 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1219 | val_micro_f1: 0.9655\n",
            "best model performance @ epoch 00195: \n",
            "\tval_loss: 0.1040 | val_micro_f1: 0.9603\n",
            "\ttest_loss: 0.0623 | test_micro_f1: 0.9759\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_75aa80c7-f255-4333-ad55-39620f3bf5c3\", \"GATv1PPI\", 29569305)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "best_model, _, _, _, _, model_saver = train(model, ppi_train_params)\n",
        "best_model = model\n",
        "model_saver.download_best_model_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHIRZuPl4f6o",
        "outputId": "c03a88ef-3f2a-4798-9a3f-14674453e3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataBatch(x=[5524, 50], edge_index=[2, 167500], y=[5524, 121], batch=[5524], ptr=[3])\n",
            "DiGraph with 5524 nodes and 167500 edges\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "[test_graph] = list(dataset.test_loader)\n",
        "test_graph_nx = to_networkx(test_graph)\n",
        "\n",
        "print(test_graph)\n",
        "print(test_graph_nx)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.0 ('grl-env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0 (default, Mar 18 2022, 03:31:43) \n[Clang 12.0.1 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "4737062b79cbd82e7f605897a0c10fa5613d72f803a33da3548714c1d9e40d67"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
