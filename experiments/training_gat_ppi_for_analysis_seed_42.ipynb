{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrEINMv8xrrb",
        "outputId": "4bb53825-d02e-409f-fa7e-81ed610ec7ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "# Check PyTorch version installed on this system\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AUKBJ8Z0neS",
        "outputId": "83f96c2b-8d8e-4c2b-8606-600d84c80cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Dec 15 22:56:51 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    31W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uWUAAH_XxxWC",
        "outputId": "9629cb3f-fbf0-4b0e-ca77-540a3d378478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TORCH=1.13.0+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 9.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.15%2Bpt113cu116-cp38-cp38-linux_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from torch-geometric) (1.0.2)\n",
            "Collecting psutil>=5.8.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 75.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=42e53149bbf90ab77b035b125179d4549691d0ee065f1d557842a9e87e0757d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/a3/20/198928106d3169865ae73afcbd3d3d1796cf6b429b55c65378\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: psutil, torch-geometric\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (2.8.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "# Download the required modules\n",
        "\"\"\"\n",
        "Assign to TORCH with what you get from the cell above, E.g., export TORCH=1.13.0+cu116\n",
        "\"\"\"\n",
        "%env TORCH=1.13.0+cu116\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric\n",
        "!pip install torchmetrics\n",
        "!pip install matplotlib\n",
        "!pip install networkx\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaHOK91EyGuR",
        "outputId": "46616afa-54d8-43e5-fb24-7becf39bdc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: github_repository=grl\n"
          ]
        }
      ],
      "source": [
        "github_username=\"deeplearningtester\"\n",
        "github_repository=\"grl\"\n",
        "github_token = \"\"\n",
        "\n",
        "%env github_repository={github_repository}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWUxyUz_x8et",
        "outputId": "40d7fb0a-a635-438f-da9c-5f329b051757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'grl'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 78 (delta 1), reused 4 (delta 1), pack-reused 70\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://{github_token}@github.com/{github_username}/{github_repository}.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtuxeTnU17rM",
        "outputId": "79ac73f4-5a91-4986-a89a-0de9d19b66c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/grl\n"
          ]
        }
      ],
      "source": [
        "%cd $github_repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6SbBZVV0Ek6",
        "outputId": "fa73bb0e-8226-41a7-dfed-0db7bde6af7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f564cbe95d0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohY0SVKP0Fb5",
        "outputId": "a7f02df0-93ff-433f-e4de-831febc7377a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
            "Extracting ./ppi.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import PPI\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.transforms import AddSelfLoops\n",
        "import dataset\n",
        "\n",
        "train_dataset = PPI(root='', split='train', transform=AddSelfLoops())\n",
        "val_dataset = PPI(root='', split='val', transform=AddSelfLoops())\n",
        "test_dataset = PPI(root='', split='test', transform=AddSelfLoops())\n",
        "\n",
        "dataset.num_features = 50\n",
        "dataset.num_labels = 121\n",
        "dataset.train_loader = DataLoader(train_dataset, batch_size=2)\n",
        "dataset.val_loader = DataLoader(val_dataset, batch_size=2)\n",
        "dataset.test_loader = DataLoader(test_dataset, batch_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IXxawUNr0Hnp"
      },
      "outputs": [],
      "source": [
        "from evaluation import evaluate\n",
        "from training_loop import train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXAdnAV20MvV",
        "outputId": "5ac919ba-24cf-4264-c1e3-c352478fc97f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.7045, device='cuda:0'), tensor(0.3902, device='cuda:0'))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models import GATv1PPI\n",
        "model = GATv1PPI(dataset.num_features, dataset.num_labels)\n",
        "evaluate(model, dataset.test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OAupMVsO0Oup"
      },
      "outputs": [],
      "source": [
        "ppi_train_params = {\n",
        "  \"lr\": 5e-3,\n",
        "  \"weight_decay\": 0,\n",
        "  \"epochs\": 400,\n",
        "  \"patience\": 100,\n",
        "  \"model_name\": model.model_name\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FG6MKVHQ0RC8",
        "outputId": "c6994f0e-9179-4148-968a-f7c0178ae88a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model GATv1PPI\n",
            "GATv1PPI(\n",
            "  (layers): ModuleList(\n",
            "    (0): GATLayer()\n",
            "    (1): GATLayer()\n",
            "    (2): GATLayer(\n",
            "      (act): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "training...\n",
            "epoch 00000\n",
            "\ttrain_loss: 0.7525 | train_micro_f1: 0.4247\n",
            "\tval_loss: 0.5892 | val_micro_f1: 0.4316\n",
            "epoch 00001\n",
            "\ttrain_loss: 0.5561 | train_micro_f1: 0.4739\n",
            "\tval_loss: 0.5164 | val_micro_f1: 0.5011\n",
            "epoch 00002\n",
            "\ttrain_loss: 0.5177 | train_micro_f1: 0.4984\n",
            "\tval_loss: 0.5001 | val_micro_f1: 0.4951\n",
            "epoch 00003\n",
            "\ttrain_loss: 0.5026 | train_micro_f1: 0.5265\n",
            "\tval_loss: 0.4895 | val_micro_f1: 0.5013\n",
            "epoch 00004\n",
            "\ttrain_loss: 0.4922 | train_micro_f1: 0.5431\n",
            "\tval_loss: 0.4807 | val_micro_f1: 0.5431\n",
            "epoch 00005\n",
            "\ttrain_loss: 0.4877 | train_micro_f1: 0.5571\n",
            "\tval_loss: 0.4823 | val_micro_f1: 0.4838\n",
            "epoch 00006\n",
            "\ttrain_loss: 0.4774 | train_micro_f1: 0.5687\n",
            "\tval_loss: 0.4703 | val_micro_f1: 0.5152\n",
            "epoch 00007\n",
            "\ttrain_loss: 0.4663 | train_micro_f1: 0.5869\n",
            "\tval_loss: 0.4658 | val_micro_f1: 0.5208\n",
            "epoch 00008\n",
            "\ttrain_loss: 0.4552 | train_micro_f1: 0.6005\n",
            "\tval_loss: 0.4522 | val_micro_f1: 0.5541\n",
            "epoch 00009\n",
            "\ttrain_loss: 0.4433 | train_micro_f1: 0.6179\n",
            "\tval_loss: 0.4438 | val_micro_f1: 0.5613\n",
            "epoch 00010\n",
            "\ttrain_loss: 0.4370 | train_micro_f1: 0.6294\n",
            "\tval_loss: 0.4499 | val_micro_f1: 0.5357\n",
            "epoch 00011\n",
            "\ttrain_loss: 0.4327 | train_micro_f1: 0.6301\n",
            "\tval_loss: 0.4280 | val_micro_f1: 0.6264\n",
            "epoch 00012\n",
            "\ttrain_loss: 0.4289 | train_micro_f1: 0.6374\n",
            "\tval_loss: 0.4280 | val_micro_f1: 0.6531\n",
            "epoch 00013\n",
            "\ttrain_loss: 0.4135 | train_micro_f1: 0.6579\n",
            "\tval_loss: 0.4084 | val_micro_f1: 0.6462\n",
            "epoch 00014\n",
            "\ttrain_loss: 0.3901 | train_micro_f1: 0.6860\n",
            "\tval_loss: 0.3911 | val_micro_f1: 0.6639\n",
            "epoch 00015\n",
            "\ttrain_loss: 0.3733 | train_micro_f1: 0.7043\n",
            "\tval_loss: 0.3818 | val_micro_f1: 0.6899\n",
            "epoch 00016\n",
            "\ttrain_loss: 0.3591 | train_micro_f1: 0.7205\n",
            "\tval_loss: 0.3677 | val_micro_f1: 0.6835\n",
            "epoch 00017\n",
            "\ttrain_loss: 0.3459 | train_micro_f1: 0.7340\n",
            "\tval_loss: 0.3564 | val_micro_f1: 0.7083\n",
            "epoch 00018\n",
            "\ttrain_loss: 0.3393 | train_micro_f1: 0.7381\n",
            "\tval_loss: 0.3500 | val_micro_f1: 0.7372\n",
            "epoch 00019\n",
            "\ttrain_loss: 0.3409 | train_micro_f1: 0.7406\n",
            "\tval_loss: 0.3549 | val_micro_f1: 0.7346\n",
            "epoch 00020\n",
            "\ttrain_loss: 0.3293 | train_micro_f1: 0.7524\n",
            "\tval_loss: 0.3369 | val_micro_f1: 0.7320\n",
            "epoch 00021\n",
            "\ttrain_loss: 0.3067 | train_micro_f1: 0.7743\n",
            "\tval_loss: 0.3200 | val_micro_f1: 0.7596\n",
            "epoch 00022\n",
            "\ttrain_loss: 0.2924 | train_micro_f1: 0.7881\n",
            "\tval_loss: 0.3107 | val_micro_f1: 0.7692\n",
            "epoch 00023\n",
            "\ttrain_loss: 0.2778 | train_micro_f1: 0.8016\n",
            "\tval_loss: 0.3000 | val_micro_f1: 0.7836\n",
            "epoch 00024\n",
            "\ttrain_loss: 0.2697 | train_micro_f1: 0.8103\n",
            "\tval_loss: 0.2917 | val_micro_f1: 0.7793\n",
            "epoch 00025\n",
            "\ttrain_loss: 0.2768 | train_micro_f1: 0.8013\n",
            "\tval_loss: 0.3126 | val_micro_f1: 0.7824\n",
            "epoch 00026\n",
            "\ttrain_loss: 0.2797 | train_micro_f1: 0.7998\n",
            "\tval_loss: 0.2926 | val_micro_f1: 0.7904\n",
            "epoch 00027\n",
            "\ttrain_loss: 0.2634 | train_micro_f1: 0.8147\n",
            "\tval_loss: 0.2841 | val_micro_f1: 0.7827\n",
            "epoch 00028\n",
            "\ttrain_loss: 0.2535 | train_micro_f1: 0.8225\n",
            "\tval_loss: 0.2724 | val_micro_f1: 0.8044\n",
            "epoch 00029\n",
            "\ttrain_loss: 0.2340 | train_micro_f1: 0.8404\n",
            "\tval_loss: 0.2606 | val_micro_f1: 0.8187\n",
            "epoch 00030\n",
            "\ttrain_loss: 0.2184 | train_micro_f1: 0.8550\n",
            "\tval_loss: 0.2501 | val_micro_f1: 0.8299\n",
            "epoch 00031\n",
            "\ttrain_loss: 0.2083 | train_micro_f1: 0.8631\n",
            "\tval_loss: 0.2405 | val_micro_f1: 0.8353\n",
            "epoch 00032\n",
            "\ttrain_loss: 0.1987 | train_micro_f1: 0.8708\n",
            "\tval_loss: 0.2336 | val_micro_f1: 0.8374\n",
            "epoch 00033\n",
            "\ttrain_loss: 0.1956 | train_micro_f1: 0.8717\n",
            "\tval_loss: 0.2333 | val_micro_f1: 0.8356\n",
            "epoch 00034\n",
            "\ttrain_loss: 0.1959 | train_micro_f1: 0.8710\n",
            "\tval_loss: 0.2349 | val_micro_f1: 0.8354\n",
            "epoch 00035\n",
            "\ttrain_loss: 0.1911 | train_micro_f1: 0.8745\n",
            "\tval_loss: 0.2343 | val_micro_f1: 0.8457\n",
            "epoch 00036\n",
            "\ttrain_loss: 0.1916 | train_micro_f1: 0.8742\n",
            "\tval_loss: 0.2313 | val_micro_f1: 0.8473\n",
            "epoch 00037\n",
            "\ttrain_loss: 0.1840 | train_micro_f1: 0.8810\n",
            "\tval_loss: 0.2191 | val_micro_f1: 0.8568\n",
            "epoch 00038\n",
            "\ttrain_loss: 0.1773 | train_micro_f1: 0.8860\n",
            "\tval_loss: 0.2133 | val_micro_f1: 0.8599\n",
            "epoch 00039\n",
            "\ttrain_loss: 0.1771 | train_micro_f1: 0.8854\n",
            "\tval_loss: 0.2128 | val_micro_f1: 0.8607\n",
            "epoch 00040\n",
            "\ttrain_loss: 0.1649 | train_micro_f1: 0.8955\n",
            "\tval_loss: 0.2098 | val_micro_f1: 0.8646\n",
            "epoch 00041\n",
            "\ttrain_loss: 0.1584 | train_micro_f1: 0.9008\n",
            "\tval_loss: 0.2022 | val_micro_f1: 0.8718\n",
            "epoch 00042\n",
            "\ttrain_loss: 0.1542 | train_micro_f1: 0.9035\n",
            "\tval_loss: 0.2004 | val_micro_f1: 0.8733\n",
            "epoch 00043\n",
            "\ttrain_loss: 0.1484 | train_micro_f1: 0.9076\n",
            "\tval_loss: 0.1950 | val_micro_f1: 0.8780\n",
            "epoch 00044\n",
            "\ttrain_loss: 0.1470 | train_micro_f1: 0.9082\n",
            "\tval_loss: 0.1897 | val_micro_f1: 0.8802\n",
            "epoch 00045\n",
            "\ttrain_loss: 0.1485 | train_micro_f1: 0.9062\n",
            "\tval_loss: 0.1916 | val_micro_f1: 0.8770\n",
            "epoch 00046\n",
            "\ttrain_loss: 0.1526 | train_micro_f1: 0.9029\n",
            "\tval_loss: 0.1947 | val_micro_f1: 0.8755\n",
            "epoch 00047\n",
            "\ttrain_loss: 0.1471 | train_micro_f1: 0.9074\n",
            "\tval_loss: 0.1908 | val_micro_f1: 0.8811\n",
            "epoch 00048\n",
            "\ttrain_loss: 0.1458 | train_micro_f1: 0.9081\n",
            "\tval_loss: 0.1859 | val_micro_f1: 0.8828\n",
            "epoch 00049\n",
            "\ttrain_loss: 0.1374 | train_micro_f1: 0.9145\n",
            "\tval_loss: 0.1797 | val_micro_f1: 0.8884\n",
            "epoch 00050\n",
            "\ttrain_loss: 0.1296 | train_micro_f1: 0.9205\n",
            "\tval_loss: 0.1731 | val_micro_f1: 0.8946\n",
            "epoch 00051\n",
            "\ttrain_loss: 0.1239 | train_micro_f1: 0.9245\n",
            "\tval_loss: 0.1749 | val_micro_f1: 0.8932\n",
            "epoch 00052\n",
            "\ttrain_loss: 0.1209 | train_micro_f1: 0.9266\n",
            "\tval_loss: 0.1709 | val_micro_f1: 0.8975\n",
            "epoch 00053\n",
            "\ttrain_loss: 0.1153 | train_micro_f1: 0.9306\n",
            "\tval_loss: 0.1646 | val_micro_f1: 0.9012\n",
            "epoch 00054\n",
            "\ttrain_loss: 0.1119 | train_micro_f1: 0.9332\n",
            "\tval_loss: 0.1631 | val_micro_f1: 0.9035\n",
            "epoch 00055\n",
            "\ttrain_loss: 0.1095 | train_micro_f1: 0.9345\n",
            "\tval_loss: 0.1637 | val_micro_f1: 0.9017\n",
            "epoch 00056\n",
            "\ttrain_loss: 0.1087 | train_micro_f1: 0.9351\n",
            "\tval_loss: 0.1660 | val_micro_f1: 0.9011\n",
            "epoch 00057\n",
            "\ttrain_loss: 0.1133 | train_micro_f1: 0.9312\n",
            "\tval_loss: 0.1671 | val_micro_f1: 0.9002\n",
            "epoch 00058\n",
            "\ttrain_loss: 0.1095 | train_micro_f1: 0.9341\n",
            "\tval_loss: 0.1652 | val_micro_f1: 0.9024\n",
            "epoch 00059\n",
            "\ttrain_loss: 0.1071 | train_micro_f1: 0.9361\n",
            "\tval_loss: 0.1652 | val_micro_f1: 0.9018\n",
            "epoch 00060\n",
            "\ttrain_loss: 0.1050 | train_micro_f1: 0.9377\n",
            "\tval_loss: 0.1624 | val_micro_f1: 0.9042\n",
            "epoch 00061\n",
            "\ttrain_loss: 0.1009 | train_micro_f1: 0.9406\n",
            "\tval_loss: 0.1567 | val_micro_f1: 0.9072\n",
            "epoch 00062\n",
            "\ttrain_loss: 0.1006 | train_micro_f1: 0.9403\n",
            "\tval_loss: 0.1534 | val_micro_f1: 0.9107\n",
            "epoch 00063\n",
            "\ttrain_loss: 0.0989 | train_micro_f1: 0.9409\n",
            "\tval_loss: 0.1492 | val_micro_f1: 0.9139\n",
            "epoch 00064\n",
            "\ttrain_loss: 0.0954 | train_micro_f1: 0.9433\n",
            "\tval_loss: 0.1498 | val_micro_f1: 0.9146\n",
            "epoch 00065\n",
            "\ttrain_loss: 0.0905 | train_micro_f1: 0.9471\n",
            "\tval_loss: 0.1439 | val_micro_f1: 0.9176\n",
            "epoch 00066\n",
            "\ttrain_loss: 0.0862 | train_micro_f1: 0.9500\n",
            "\tval_loss: 0.1440 | val_micro_f1: 0.9191\n",
            "epoch 00067\n",
            "\ttrain_loss: 0.0846 | train_micro_f1: 0.9510\n",
            "\tval_loss: 0.1429 | val_micro_f1: 0.9179\n",
            "epoch 00068\n",
            "\ttrain_loss: 0.0862 | train_micro_f1: 0.9493\n",
            "\tval_loss: 0.1464 | val_micro_f1: 0.9177\n",
            "epoch 00069\n",
            "\ttrain_loss: 0.0874 | train_micro_f1: 0.9485\n",
            "\tval_loss: 0.1458 | val_micro_f1: 0.9162\n",
            "epoch 00070\n",
            "\ttrain_loss: 0.0900 | train_micro_f1: 0.9463\n",
            "\tval_loss: 0.1496 | val_micro_f1: 0.9159\n",
            "epoch 00071\n",
            "\ttrain_loss: 0.0903 | train_micro_f1: 0.9458\n",
            "\tval_loss: 0.1498 | val_micro_f1: 0.9123\n",
            "epoch 00072\n",
            "\ttrain_loss: 0.0847 | train_micro_f1: 0.9501\n",
            "\tval_loss: 0.1447 | val_micro_f1: 0.9191\n",
            "epoch 00073\n",
            "\ttrain_loss: 0.0823 | train_micro_f1: 0.9518\n",
            "\tval_loss: 0.1467 | val_micro_f1: 0.9162\n",
            "epoch 00074\n",
            "\ttrain_loss: 0.0791 | train_micro_f1: 0.9539\n",
            "\tval_loss: 0.1377 | val_micro_f1: 0.9240\n",
            "epoch 00075\n",
            "\ttrain_loss: 0.0779 | train_micro_f1: 0.9546\n",
            "\tval_loss: 0.1373 | val_micro_f1: 0.9242\n",
            "epoch 00076\n",
            "\ttrain_loss: 0.0777 | train_micro_f1: 0.9545\n",
            "\tval_loss: 0.1413 | val_micro_f1: 0.9222\n",
            "epoch 00077\n",
            "\ttrain_loss: 0.0787 | train_micro_f1: 0.9544\n",
            "\tval_loss: 0.1407 | val_micro_f1: 0.9228\n",
            "epoch 00078\n",
            "\ttrain_loss: 0.0847 | train_micro_f1: 0.9498\n",
            "\tval_loss: 0.1598 | val_micro_f1: 0.9110\n",
            "epoch 00079\n",
            "\ttrain_loss: 0.0892 | train_micro_f1: 0.9469\n",
            "\tval_loss: 0.1475 | val_micro_f1: 0.9185\n",
            "epoch 00080\n",
            "\ttrain_loss: 0.0892 | train_micro_f1: 0.9462\n",
            "\tval_loss: 0.1496 | val_micro_f1: 0.9182\n",
            "epoch 00081\n",
            "\ttrain_loss: 0.0896 | train_micro_f1: 0.9459\n",
            "\tval_loss: 0.1521 | val_micro_f1: 0.9166\n",
            "epoch 00082\n",
            "\ttrain_loss: 0.0852 | train_micro_f1: 0.9493\n",
            "\tval_loss: 0.1447 | val_micro_f1: 0.9207\n",
            "epoch 00083\n",
            "\ttrain_loss: 0.0788 | train_micro_f1: 0.9537\n",
            "\tval_loss: 0.1361 | val_micro_f1: 0.9256\n",
            "epoch 00084\n",
            "\ttrain_loss: 0.0704 | train_micro_f1: 0.9594\n",
            "\tval_loss: 0.1340 | val_micro_f1: 0.9266\n",
            "epoch 00085\n",
            "\ttrain_loss: 0.0679 | train_micro_f1: 0.9610\n",
            "\tval_loss: 0.1345 | val_micro_f1: 0.9267\n",
            "epoch 00086\n",
            "\ttrain_loss: 0.0673 | train_micro_f1: 0.9613\n",
            "\tval_loss: 0.1358 | val_micro_f1: 0.9261\n",
            "epoch 00087\n",
            "\ttrain_loss: 0.0660 | train_micro_f1: 0.9620\n",
            "\tval_loss: 0.1287 | val_micro_f1: 0.9324\n",
            "epoch 00088\n",
            "\ttrain_loss: 0.0713 | train_micro_f1: 0.9583\n",
            "\tval_loss: 0.1427 | val_micro_f1: 0.9247\n",
            "epoch 00089\n",
            "\ttrain_loss: 0.0761 | train_micro_f1: 0.9550\n",
            "\tval_loss: 0.1454 | val_micro_f1: 0.9217\n",
            "epoch 00090\n",
            "\ttrain_loss: 0.0829 | train_micro_f1: 0.9506\n",
            "\tval_loss: 0.1441 | val_micro_f1: 0.9227\n",
            "epoch 00091\n",
            "\ttrain_loss: 0.0818 | train_micro_f1: 0.9516\n",
            "\tval_loss: 0.1450 | val_micro_f1: 0.9210\n",
            "epoch 00092\n",
            "\ttrain_loss: 0.0762 | train_micro_f1: 0.9547\n",
            "\tval_loss: 0.1354 | val_micro_f1: 0.9277\n",
            "epoch 00093\n",
            "\ttrain_loss: 0.0705 | train_micro_f1: 0.9586\n",
            "\tval_loss: 0.1310 | val_micro_f1: 0.9308\n",
            "epoch 00094\n",
            "\ttrain_loss: 0.0697 | train_micro_f1: 0.9590\n",
            "\tval_loss: 0.1323 | val_micro_f1: 0.9307\n",
            "epoch 00095\n",
            "\ttrain_loss: 0.0635 | train_micro_f1: 0.9637\n",
            "\tval_loss: 0.1252 | val_micro_f1: 0.9360\n",
            "epoch 00096\n",
            "\ttrain_loss: 0.0556 | train_micro_f1: 0.9690\n",
            "\tval_loss: 0.1210 | val_micro_f1: 0.9391\n",
            "epoch 00097\n",
            "\ttrain_loss: 0.0510 | train_micro_f1: 0.9718\n",
            "\tval_loss: 0.1177 | val_micro_f1: 0.9411\n",
            "epoch 00098\n",
            "\ttrain_loss: 0.0479 | train_micro_f1: 0.9738\n",
            "\tval_loss: 0.1158 | val_micro_f1: 0.9421\n",
            "epoch 00099\n",
            "\ttrain_loss: 0.0472 | train_micro_f1: 0.9739\n",
            "\tval_loss: 0.1137 | val_micro_f1: 0.9432\n",
            "epoch 00100\n",
            "\ttrain_loss: 0.0511 | train_micro_f1: 0.9714\n",
            "\tval_loss: 0.1171 | val_micro_f1: 0.9414\n",
            "epoch 00101\n",
            "\ttrain_loss: 0.0525 | train_micro_f1: 0.9704\n",
            "\tval_loss: 0.1252 | val_micro_f1: 0.9374\n",
            "epoch 00102\n",
            "\ttrain_loss: 0.0531 | train_micro_f1: 0.9699\n",
            "\tval_loss: 0.1243 | val_micro_f1: 0.9382\n",
            "epoch 00103\n",
            "\ttrain_loss: 0.0542 | train_micro_f1: 0.9695\n",
            "\tval_loss: 0.1219 | val_micro_f1: 0.9399\n",
            "epoch 00104\n",
            "\ttrain_loss: 0.0517 | train_micro_f1: 0.9710\n",
            "\tval_loss: 0.1179 | val_micro_f1: 0.9420\n",
            "epoch 00105\n",
            "\ttrain_loss: 0.0507 | train_micro_f1: 0.9715\n",
            "\tval_loss: 0.1212 | val_micro_f1: 0.9408\n",
            "epoch 00106\n",
            "\ttrain_loss: 0.0470 | train_micro_f1: 0.9743\n",
            "\tval_loss: 0.1156 | val_micro_f1: 0.9437\n",
            "epoch 00107\n",
            "\ttrain_loss: 0.0433 | train_micro_f1: 0.9765\n",
            "\tval_loss: 0.1101 | val_micro_f1: 0.9466\n",
            "epoch 00108\n",
            "\ttrain_loss: 0.0405 | train_micro_f1: 0.9785\n",
            "\tval_loss: 0.1097 | val_micro_f1: 0.9478\n",
            "epoch 00109\n",
            "\ttrain_loss: 0.0377 | train_micro_f1: 0.9804\n",
            "\tval_loss: 0.1074 | val_micro_f1: 0.9491\n",
            "epoch 00110\n",
            "\ttrain_loss: 0.0369 | train_micro_f1: 0.9807\n",
            "\tval_loss: 0.1090 | val_micro_f1: 0.9487\n",
            "epoch 00111\n",
            "\ttrain_loss: 0.0362 | train_micro_f1: 0.9813\n",
            "\tval_loss: 0.1070 | val_micro_f1: 0.9502\n",
            "epoch 00112\n",
            "\ttrain_loss: 0.0360 | train_micro_f1: 0.9812\n",
            "\tval_loss: 0.1075 | val_micro_f1: 0.9499\n",
            "epoch 00113\n",
            "\ttrain_loss: 0.0354 | train_micro_f1: 0.9814\n",
            "\tval_loss: 0.1094 | val_micro_f1: 0.9493\n",
            "epoch 00114\n",
            "\ttrain_loss: 0.0378 | train_micro_f1: 0.9797\n",
            "\tval_loss: 0.1094 | val_micro_f1: 0.9496\n",
            "epoch 00115\n",
            "\ttrain_loss: 0.0384 | train_micro_f1: 0.9794\n",
            "\tval_loss: 0.1167 | val_micro_f1: 0.9446\n",
            "epoch 00116\n",
            "\ttrain_loss: 0.0407 | train_micro_f1: 0.9782\n",
            "\tval_loss: 0.1163 | val_micro_f1: 0.9452\n",
            "epoch 00117\n",
            "\ttrain_loss: 0.0397 | train_micro_f1: 0.9784\n",
            "\tval_loss: 0.1135 | val_micro_f1: 0.9476\n",
            "epoch 00118\n",
            "\ttrain_loss: 0.0390 | train_micro_f1: 0.9791\n",
            "\tval_loss: 0.1130 | val_micro_f1: 0.9481\n",
            "epoch 00119\n",
            "\ttrain_loss: 0.0403 | train_micro_f1: 0.9784\n",
            "\tval_loss: 0.1134 | val_micro_f1: 0.9466\n",
            "epoch 00120\n",
            "\ttrain_loss: 0.0401 | train_micro_f1: 0.9781\n",
            "\tval_loss: 0.1136 | val_micro_f1: 0.9475\n",
            "epoch 00121\n",
            "\ttrain_loss: 0.0399 | train_micro_f1: 0.9782\n",
            "\tval_loss: 0.1146 | val_micro_f1: 0.9471\n",
            "epoch 00122\n",
            "\ttrain_loss: 0.0429 | train_micro_f1: 0.9760\n",
            "\tval_loss: 0.1165 | val_micro_f1: 0.9463\n",
            "epoch 00123\n",
            "\ttrain_loss: 0.0459 | train_micro_f1: 0.9740\n",
            "\tval_loss: 0.1172 | val_micro_f1: 0.9453\n",
            "epoch 00124\n",
            "\ttrain_loss: 0.0464 | train_micro_f1: 0.9737\n",
            "\tval_loss: 0.1181 | val_micro_f1: 0.9446\n",
            "epoch 00125\n",
            "\ttrain_loss: 0.0490 | train_micro_f1: 0.9721\n",
            "\tval_loss: 0.1224 | val_micro_f1: 0.9436\n",
            "epoch 00126\n",
            "\ttrain_loss: 0.0506 | train_micro_f1: 0.9711\n",
            "\tval_loss: 0.1240 | val_micro_f1: 0.9427\n",
            "epoch 00127\n",
            "\ttrain_loss: 0.0487 | train_micro_f1: 0.9723\n",
            "\tval_loss: 0.1252 | val_micro_f1: 0.9406\n",
            "epoch 00128\n",
            "\ttrain_loss: 0.0460 | train_micro_f1: 0.9738\n",
            "\tval_loss: 0.1191 | val_micro_f1: 0.9443\n",
            "epoch 00129\n",
            "\ttrain_loss: 0.0418 | train_micro_f1: 0.9764\n",
            "\tval_loss: 0.1188 | val_micro_f1: 0.9453\n",
            "epoch 00130\n",
            "\ttrain_loss: 0.0388 | train_micro_f1: 0.9787\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9494\n",
            "epoch 00131\n",
            "\ttrain_loss: 0.0358 | train_micro_f1: 0.9808\n",
            "\tval_loss: 0.1097 | val_micro_f1: 0.9505\n",
            "epoch 00132\n",
            "\ttrain_loss: 0.0323 | train_micro_f1: 0.9826\n",
            "\tval_loss: 0.1076 | val_micro_f1: 0.9519\n",
            "epoch 00133\n",
            "\ttrain_loss: 0.0301 | train_micro_f1: 0.9843\n",
            "\tval_loss: 0.1079 | val_micro_f1: 0.9527\n",
            "epoch 00134\n",
            "\ttrain_loss: 0.0294 | train_micro_f1: 0.9850\n",
            "\tval_loss: 0.1061 | val_micro_f1: 0.9536\n",
            "epoch 00135\n",
            "\ttrain_loss: 0.0280 | train_micro_f1: 0.9859\n",
            "\tval_loss: 0.1078 | val_micro_f1: 0.9529\n",
            "epoch 00136\n",
            "\ttrain_loss: 0.0268 | train_micro_f1: 0.9868\n",
            "\tval_loss: 0.1081 | val_micro_f1: 0.9530\n",
            "epoch 00137\n",
            "\ttrain_loss: 0.0254 | train_micro_f1: 0.9874\n",
            "\tval_loss: 0.1042 | val_micro_f1: 0.9555\n",
            "epoch 00138\n",
            "\ttrain_loss: 0.0247 | train_micro_f1: 0.9879\n",
            "\tval_loss: 0.1053 | val_micro_f1: 0.9548\n",
            "epoch 00139\n",
            "\ttrain_loss: 0.0243 | train_micro_f1: 0.9881\n",
            "\tval_loss: 0.1062 | val_micro_f1: 0.9556\n",
            "epoch 00140\n",
            "\ttrain_loss: 0.0240 | train_micro_f1: 0.9881\n",
            "\tval_loss: 0.1072 | val_micro_f1: 0.9542\n",
            "epoch 00141\n",
            "\ttrain_loss: 0.0246 | train_micro_f1: 0.9879\n",
            "\tval_loss: 0.1104 | val_micro_f1: 0.9527\n",
            "epoch 00142\n",
            "\ttrain_loss: 0.0246 | train_micro_f1: 0.9877\n",
            "\tval_loss: 0.1079 | val_micro_f1: 0.9539\n",
            "epoch 00143\n",
            "\ttrain_loss: 0.0253 | train_micro_f1: 0.9873\n",
            "\tval_loss: 0.1080 | val_micro_f1: 0.9543\n",
            "epoch 00144\n",
            "\ttrain_loss: 0.0261 | train_micro_f1: 0.9870\n",
            "\tval_loss: 0.1089 | val_micro_f1: 0.9537\n",
            "epoch 00145\n",
            "\ttrain_loss: 0.0257 | train_micro_f1: 0.9870\n",
            "\tval_loss: 0.1071 | val_micro_f1: 0.9546\n",
            "epoch 00146\n",
            "\ttrain_loss: 0.0289 | train_micro_f1: 0.9851\n",
            "\tval_loss: 0.1112 | val_micro_f1: 0.9534\n",
            "epoch 00147\n",
            "\ttrain_loss: 0.0295 | train_micro_f1: 0.9852\n",
            "\tval_loss: 0.1146 | val_micro_f1: 0.9520\n",
            "epoch 00148\n",
            "\ttrain_loss: 0.0278 | train_micro_f1: 0.9857\n",
            "\tval_loss: 0.1087 | val_micro_f1: 0.9546\n",
            "epoch 00149\n",
            "\ttrain_loss: 0.0281 | train_micro_f1: 0.9851\n",
            "\tval_loss: 0.1093 | val_micro_f1: 0.9547\n",
            "epoch 00150\n",
            "\ttrain_loss: 0.0289 | train_micro_f1: 0.9849\n",
            "\tval_loss: 0.1074 | val_micro_f1: 0.9558\n",
            "epoch 00151\n",
            "\ttrain_loss: 0.0286 | train_micro_f1: 0.9850\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9533\n",
            "epoch 00152\n",
            "\ttrain_loss: 0.0283 | train_micro_f1: 0.9849\n",
            "\tval_loss: 0.1097 | val_micro_f1: 0.9538\n",
            "epoch 00153\n",
            "\ttrain_loss: 0.0286 | train_micro_f1: 0.9847\n",
            "\tval_loss: 0.1107 | val_micro_f1: 0.9547\n",
            "epoch 00154\n",
            "\ttrain_loss: 0.0263 | train_micro_f1: 0.9863\n",
            "\tval_loss: 0.1100 | val_micro_f1: 0.9548\n",
            "epoch 00155\n",
            "\ttrain_loss: 0.0248 | train_micro_f1: 0.9874\n",
            "\tval_loss: 0.1097 | val_micro_f1: 0.9552\n",
            "epoch 00156\n",
            "\ttrain_loss: 0.0254 | train_micro_f1: 0.9870\n",
            "\tval_loss: 0.1110 | val_micro_f1: 0.9550\n",
            "epoch 00157\n",
            "\ttrain_loss: 0.0244 | train_micro_f1: 0.9882\n",
            "\tval_loss: 0.1078 | val_micro_f1: 0.9565\n",
            "epoch 00158\n",
            "\ttrain_loss: 0.0216 | train_micro_f1: 0.9894\n",
            "\tval_loss: 0.1078 | val_micro_f1: 0.9562\n",
            "epoch 00159\n",
            "\ttrain_loss: 0.0225 | train_micro_f1: 0.9889\n",
            "\tval_loss: 0.1092 | val_micro_f1: 0.9557\n",
            "epoch 00160\n",
            "\ttrain_loss: 0.0223 | train_micro_f1: 0.9891\n",
            "\tval_loss: 0.1098 | val_micro_f1: 0.9559\n",
            "epoch 00161\n",
            "\ttrain_loss: 0.0233 | train_micro_f1: 0.9886\n",
            "\tval_loss: 0.1095 | val_micro_f1: 0.9561\n",
            "epoch 00162\n",
            "\ttrain_loss: 0.0223 | train_micro_f1: 0.9888\n",
            "\tval_loss: 0.1090 | val_micro_f1: 0.9567\n",
            "epoch 00163\n",
            "\ttrain_loss: 0.0241 | train_micro_f1: 0.9882\n",
            "\tval_loss: 0.1087 | val_micro_f1: 0.9569\n",
            "epoch 00164\n",
            "\ttrain_loss: 0.0245 | train_micro_f1: 0.9875\n",
            "\tval_loss: 0.1103 | val_micro_f1: 0.9562\n",
            "epoch 00165\n",
            "\ttrain_loss: 0.0251 | train_micro_f1: 0.9877\n",
            "\tval_loss: 0.1097 | val_micro_f1: 0.9565\n",
            "epoch 00166\n",
            "\ttrain_loss: 0.0265 | train_micro_f1: 0.9862\n",
            "\tval_loss: 0.1103 | val_micro_f1: 0.9562\n",
            "epoch 00167\n",
            "\ttrain_loss: 0.0257 | train_micro_f1: 0.9871\n",
            "\tval_loss: 0.1107 | val_micro_f1: 0.9558\n",
            "epoch 00168\n",
            "\ttrain_loss: 0.0262 | train_micro_f1: 0.9865\n",
            "\tval_loss: 0.1107 | val_micro_f1: 0.9565\n",
            "epoch 00169\n",
            "\ttrain_loss: 0.0242 | train_micro_f1: 0.9882\n",
            "\tval_loss: 0.1097 | val_micro_f1: 0.9572\n",
            "epoch 00170\n",
            "\ttrain_loss: 0.0215 | train_micro_f1: 0.9892\n",
            "\tval_loss: 0.1091 | val_micro_f1: 0.9583\n",
            "epoch 00171\n",
            "\ttrain_loss: 0.0211 | train_micro_f1: 0.9899\n",
            "\tval_loss: 0.1091 | val_micro_f1: 0.9579\n",
            "epoch 00172\n",
            "\ttrain_loss: 0.0202 | train_micro_f1: 0.9899\n",
            "\tval_loss: 0.1090 | val_micro_f1: 0.9585\n",
            "epoch 00173\n",
            "\ttrain_loss: 0.0204 | train_micro_f1: 0.9902\n",
            "\tval_loss: 0.1086 | val_micro_f1: 0.9582\n",
            "epoch 00174\n",
            "\ttrain_loss: 0.0203 | train_micro_f1: 0.9897\n",
            "\tval_loss: 0.1100 | val_micro_f1: 0.9575\n",
            "epoch 00175\n",
            "\ttrain_loss: 0.0223 | train_micro_f1: 0.9889\n",
            "\tval_loss: 0.1136 | val_micro_f1: 0.9561\n",
            "epoch 00176\n",
            "\ttrain_loss: 0.0220 | train_micro_f1: 0.9888\n",
            "\tval_loss: 0.1125 | val_micro_f1: 0.9565\n",
            "epoch 00177\n",
            "\ttrain_loss: 0.0233 | train_micro_f1: 0.9884\n",
            "\tval_loss: 0.1116 | val_micro_f1: 0.9562\n",
            "epoch 00178\n",
            "\ttrain_loss: 0.0241 | train_micro_f1: 0.9871\n",
            "\tval_loss: 0.1120 | val_micro_f1: 0.9555\n",
            "epoch 00179\n",
            "\ttrain_loss: 0.0259 | train_micro_f1: 0.9862\n",
            "\tval_loss: 0.1126 | val_micro_f1: 0.9561\n",
            "epoch 00180\n",
            "\ttrain_loss: 0.0295 | train_micro_f1: 0.9839\n",
            "\tval_loss: 0.1177 | val_micro_f1: 0.9531\n",
            "epoch 00181\n",
            "\ttrain_loss: 0.0315 | train_micro_f1: 0.9827\n",
            "\tval_loss: 0.1271 | val_micro_f1: 0.9493\n",
            "epoch 00182\n",
            "\ttrain_loss: 0.0406 | train_micro_f1: 0.9770\n",
            "\tval_loss: 0.1295 | val_micro_f1: 0.9445\n",
            "epoch 00183\n",
            "\ttrain_loss: 0.0441 | train_micro_f1: 0.9752\n",
            "\tval_loss: 0.1327 | val_micro_f1: 0.9454\n",
            "epoch 00184\n",
            "\ttrain_loss: 0.0540 | train_micro_f1: 0.9697\n",
            "\tval_loss: 0.1431 | val_micro_f1: 0.9365\n",
            "epoch 00185\n",
            "\ttrain_loss: 0.0585 | train_micro_f1: 0.9666\n",
            "\tval_loss: 0.1364 | val_micro_f1: 0.9424\n",
            "epoch 00186\n",
            "\ttrain_loss: 0.0771 | train_micro_f1: 0.9568\n",
            "\tval_loss: 0.1702 | val_micro_f1: 0.9247\n",
            "epoch 00187\n",
            "\ttrain_loss: 0.0807 | train_micro_f1: 0.9553\n",
            "\tval_loss: 0.1541 | val_micro_f1: 0.9316\n",
            "epoch 00188\n",
            "\ttrain_loss: 0.0677 | train_micro_f1: 0.9624\n",
            "\tval_loss: 0.1416 | val_micro_f1: 0.9392\n",
            "epoch 00189\n",
            "\ttrain_loss: 0.0513 | train_micro_f1: 0.9712\n",
            "\tval_loss: 0.1275 | val_micro_f1: 0.9460\n",
            "epoch 00190\n",
            "\ttrain_loss: 0.0435 | train_micro_f1: 0.9767\n",
            "\tval_loss: 0.1239 | val_micro_f1: 0.9497\n",
            "epoch 00191\n",
            "\ttrain_loss: 0.0346 | train_micro_f1: 0.9817\n",
            "\tval_loss: 0.1180 | val_micro_f1: 0.9535\n",
            "epoch 00192\n",
            "\ttrain_loss: 0.0269 | train_micro_f1: 0.9858\n",
            "\tval_loss: 0.1134 | val_micro_f1: 0.9555\n",
            "epoch 00193\n",
            "\ttrain_loss: 0.0216 | train_micro_f1: 0.9891\n",
            "\tval_loss: 0.1054 | val_micro_f1: 0.9601\n",
            "epoch 00194\n",
            "\ttrain_loss: 0.0186 | train_micro_f1: 0.9907\n",
            "\tval_loss: 0.1026 | val_micro_f1: 0.9619\n",
            "epoch 00195\n",
            "\ttrain_loss: 0.0178 | train_micro_f1: 0.9918\n",
            "\tval_loss: 0.1013 | val_micro_f1: 0.9623\n",
            "epoch 00196\n",
            "\ttrain_loss: 0.0162 | train_micro_f1: 0.9924\n",
            "\tval_loss: 0.1039 | val_micro_f1: 0.9623\n",
            "epoch 00197\n",
            "\ttrain_loss: 0.0159 | train_micro_f1: 0.9928\n",
            "\tval_loss: 0.1047 | val_micro_f1: 0.9619\n",
            "epoch 00198\n",
            "\ttrain_loss: 0.0152 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1031 | val_micro_f1: 0.9628\n",
            "epoch 00199\n",
            "\ttrain_loss: 0.0148 | train_micro_f1: 0.9932\n",
            "\tval_loss: 0.1022 | val_micro_f1: 0.9624\n",
            "epoch 00200\n",
            "\ttrain_loss: 0.0157 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1018 | val_micro_f1: 0.9633\n",
            "epoch 00201\n",
            "\ttrain_loss: 0.0161 | train_micro_f1: 0.9927\n",
            "\tval_loss: 0.1060 | val_micro_f1: 0.9618\n",
            "epoch 00202\n",
            "\ttrain_loss: 0.0167 | train_micro_f1: 0.9928\n",
            "\tval_loss: 0.1069 | val_micro_f1: 0.9619\n",
            "epoch 00203\n",
            "\ttrain_loss: 0.0170 | train_micro_f1: 0.9920\n",
            "\tval_loss: 0.1107 | val_micro_f1: 0.9602\n",
            "epoch 00204\n",
            "\ttrain_loss: 0.0184 | train_micro_f1: 0.9923\n",
            "\tval_loss: 0.1084 | val_micro_f1: 0.9606\n",
            "epoch 00205\n",
            "\ttrain_loss: 0.0174 | train_micro_f1: 0.9916\n",
            "\tval_loss: 0.1106 | val_micro_f1: 0.9600\n",
            "epoch 00206\n",
            "\ttrain_loss: 0.0181 | train_micro_f1: 0.9918\n",
            "\tval_loss: 0.1093 | val_micro_f1: 0.9600\n",
            "epoch 00207\n",
            "\ttrain_loss: 0.0176 | train_micro_f1: 0.9912\n",
            "\tval_loss: 0.1102 | val_micro_f1: 0.9599\n",
            "epoch 00208\n",
            "\ttrain_loss: 0.0187 | train_micro_f1: 0.9913\n",
            "\tval_loss: 0.1119 | val_micro_f1: 0.9592\n",
            "epoch 00209\n",
            "\ttrain_loss: 0.0194 | train_micro_f1: 0.9903\n",
            "\tval_loss: 0.1147 | val_micro_f1: 0.9583\n",
            "epoch 00210\n",
            "\ttrain_loss: 0.0201 | train_micro_f1: 0.9905\n",
            "\tval_loss: 0.1104 | val_micro_f1: 0.9605\n",
            "epoch 00211\n",
            "\ttrain_loss: 0.0184 | train_micro_f1: 0.9910\n",
            "\tval_loss: 0.1092 | val_micro_f1: 0.9612\n",
            "epoch 00212\n",
            "\ttrain_loss: 0.0175 | train_micro_f1: 0.9918\n",
            "\tval_loss: 0.1103 | val_micro_f1: 0.9609\n",
            "epoch 00213\n",
            "\ttrain_loss: 0.0165 | train_micro_f1: 0.9919\n",
            "\tval_loss: 0.1117 | val_micro_f1: 0.9604\n",
            "epoch 00214\n",
            "\ttrain_loss: 0.0159 | train_micro_f1: 0.9928\n",
            "\tval_loss: 0.1091 | val_micro_f1: 0.9616\n",
            "epoch 00215\n",
            "\ttrain_loss: 0.0151 | train_micro_f1: 0.9926\n",
            "\tval_loss: 0.1073 | val_micro_f1: 0.9624\n",
            "epoch 00216\n",
            "\ttrain_loss: 0.0153 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1041 | val_micro_f1: 0.9631\n",
            "epoch 00217\n",
            "\ttrain_loss: 0.0139 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1069 | val_micro_f1: 0.9626\n",
            "epoch 00218\n",
            "\ttrain_loss: 0.0142 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1065 | val_micro_f1: 0.9635\n",
            "epoch 00219\n",
            "\ttrain_loss: 0.0135 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1066 | val_micro_f1: 0.9637\n",
            "epoch 00220\n",
            "\ttrain_loss: 0.0131 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1057 | val_micro_f1: 0.9644\n",
            "epoch 00221\n",
            "\ttrain_loss: 0.0130 | train_micro_f1: 0.9938\n",
            "\tval_loss: 0.1058 | val_micro_f1: 0.9645\n",
            "epoch 00222\n",
            "\ttrain_loss: 0.0139 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1064 | val_micro_f1: 0.9634\n",
            "epoch 00223\n",
            "\ttrain_loss: 0.0138 | train_micro_f1: 0.9938\n",
            "\tval_loss: 0.1070 | val_micro_f1: 0.9638\n",
            "epoch 00224\n",
            "\ttrain_loss: 0.0136 | train_micro_f1: 0.9941\n",
            "\tval_loss: 0.1087 | val_micro_f1: 0.9628\n",
            "epoch 00225\n",
            "\ttrain_loss: 0.0138 | train_micro_f1: 0.9937\n",
            "\tval_loss: 0.1088 | val_micro_f1: 0.9628\n",
            "epoch 00226\n",
            "\ttrain_loss: 0.0131 | train_micro_f1: 0.9941\n",
            "\tval_loss: 0.1098 | val_micro_f1: 0.9627\n",
            "epoch 00227\n",
            "\ttrain_loss: 0.0132 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1096 | val_micro_f1: 0.9632\n",
            "epoch 00228\n",
            "\ttrain_loss: 0.0136 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1082 | val_micro_f1: 0.9638\n",
            "epoch 00229\n",
            "\ttrain_loss: 0.0139 | train_micro_f1: 0.9935\n",
            "\tval_loss: 0.1095 | val_micro_f1: 0.9628\n",
            "epoch 00230\n",
            "\ttrain_loss: 0.0135 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1090 | val_micro_f1: 0.9631\n",
            "epoch 00231\n",
            "\ttrain_loss: 0.0144 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1087 | val_micro_f1: 0.9628\n",
            "epoch 00232\n",
            "\ttrain_loss: 0.0145 | train_micro_f1: 0.9930\n",
            "\tval_loss: 0.1106 | val_micro_f1: 0.9623\n",
            "epoch 00233\n",
            "\ttrain_loss: 0.0152 | train_micro_f1: 0.9931\n",
            "\tval_loss: 0.1105 | val_micro_f1: 0.9628\n",
            "epoch 00234\n",
            "\ttrain_loss: 0.0148 | train_micro_f1: 0.9930\n",
            "\tval_loss: 0.1104 | val_micro_f1: 0.9630\n",
            "epoch 00235\n",
            "\ttrain_loss: 0.0151 | train_micro_f1: 0.9934\n",
            "\tval_loss: 0.1088 | val_micro_f1: 0.9631\n",
            "epoch 00236\n",
            "\ttrain_loss: 0.0130 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1087 | val_micro_f1: 0.9634\n",
            "epoch 00237\n",
            "\ttrain_loss: 0.0134 | train_micro_f1: 0.9937\n",
            "\tval_loss: 0.1091 | val_micro_f1: 0.9635\n",
            "epoch 00238\n",
            "\ttrain_loss: 0.0133 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1108 | val_micro_f1: 0.9635\n",
            "epoch 00239\n",
            "\ttrain_loss: 0.0129 | train_micro_f1: 0.9941\n",
            "\tval_loss: 0.1092 | val_micro_f1: 0.9639\n",
            "epoch 00240\n",
            "\ttrain_loss: 0.0130 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1076 | val_micro_f1: 0.9645\n",
            "epoch 00241\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1077 | val_micro_f1: 0.9652\n",
            "epoch 00242\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1079 | val_micro_f1: 0.9649\n",
            "epoch 00243\n",
            "\ttrain_loss: 0.0132 | train_micro_f1: 0.9937\n",
            "\tval_loss: 0.1111 | val_micro_f1: 0.9635\n",
            "epoch 00244\n",
            "\ttrain_loss: 0.0144 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1151 | val_micro_f1: 0.9613\n",
            "epoch 00245\n",
            "\ttrain_loss: 0.0140 | train_micro_f1: 0.9931\n",
            "\tval_loss: 0.1157 | val_micro_f1: 0.9620\n",
            "epoch 00246\n",
            "\ttrain_loss: 0.0150 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1161 | val_micro_f1: 0.9614\n",
            "epoch 00247\n",
            "\ttrain_loss: 0.0147 | train_micro_f1: 0.9925\n",
            "\tval_loss: 0.1159 | val_micro_f1: 0.9625\n",
            "epoch 00248\n",
            "\ttrain_loss: 0.0159 | train_micro_f1: 0.9928\n",
            "\tval_loss: 0.1150 | val_micro_f1: 0.9618\n",
            "epoch 00249\n",
            "\ttrain_loss: 0.0156 | train_micro_f1: 0.9920\n",
            "\tval_loss: 0.1170 | val_micro_f1: 0.9616\n",
            "epoch 00250\n",
            "\ttrain_loss: 0.0172 | train_micro_f1: 0.9920\n",
            "\tval_loss: 0.1168 | val_micro_f1: 0.9601\n",
            "epoch 00251\n",
            "\ttrain_loss: 0.0170 | train_micro_f1: 0.9912\n",
            "\tval_loss: 0.1186 | val_micro_f1: 0.9599\n",
            "epoch 00252\n",
            "\ttrain_loss: 0.0176 | train_micro_f1: 0.9911\n",
            "\tval_loss: 0.1182 | val_micro_f1: 0.9593\n",
            "epoch 00253\n",
            "\ttrain_loss: 0.0182 | train_micro_f1: 0.9905\n",
            "\tval_loss: 0.1212 | val_micro_f1: 0.9580\n",
            "epoch 00254\n",
            "\ttrain_loss: 0.0188 | train_micro_f1: 0.9905\n",
            "\tval_loss: 0.1186 | val_micro_f1: 0.9601\n",
            "epoch 00255\n",
            "\ttrain_loss: 0.0180 | train_micro_f1: 0.9906\n",
            "\tval_loss: 0.1158 | val_micro_f1: 0.9611\n",
            "epoch 00256\n",
            "\ttrain_loss: 0.0196 | train_micro_f1: 0.9900\n",
            "\tval_loss: 0.1182 | val_micro_f1: 0.9599\n",
            "epoch 00257\n",
            "\ttrain_loss: 0.0189 | train_micro_f1: 0.9900\n",
            "\tval_loss: 0.1192 | val_micro_f1: 0.9605\n",
            "epoch 00258\n",
            "\ttrain_loss: 0.0201 | train_micro_f1: 0.9896\n",
            "\tval_loss: 0.1177 | val_micro_f1: 0.9611\n",
            "epoch 00259\n",
            "\ttrain_loss: 0.0204 | train_micro_f1: 0.9890\n",
            "\tval_loss: 0.1206 | val_micro_f1: 0.9594\n",
            "epoch 00260\n",
            "\ttrain_loss: 0.0252 | train_micro_f1: 0.9864\n",
            "\tval_loss: 0.1198 | val_micro_f1: 0.9588\n",
            "epoch 00261\n",
            "\ttrain_loss: 0.0301 | train_micro_f1: 0.9836\n",
            "\tval_loss: 0.1290 | val_micro_f1: 0.9551\n",
            "epoch 00262\n",
            "\ttrain_loss: 0.0355 | train_micro_f1: 0.9809\n",
            "\tval_loss: 0.1335 | val_micro_f1: 0.9500\n",
            "epoch 00263\n",
            "\ttrain_loss: 0.0425 | train_micro_f1: 0.9767\n",
            "\tval_loss: 0.1344 | val_micro_f1: 0.9507\n",
            "epoch 00264\n",
            "\ttrain_loss: 0.0439 | train_micro_f1: 0.9756\n",
            "\tval_loss: 0.1407 | val_micro_f1: 0.9472\n",
            "epoch 00265\n",
            "\ttrain_loss: 0.0458 | train_micro_f1: 0.9745\n",
            "\tval_loss: 0.1399 | val_micro_f1: 0.9469\n",
            "epoch 00266\n",
            "\ttrain_loss: 0.0474 | train_micro_f1: 0.9743\n",
            "\tval_loss: 0.1383 | val_micro_f1: 0.9474\n",
            "epoch 00267\n",
            "\ttrain_loss: 0.0462 | train_micro_f1: 0.9745\n",
            "\tval_loss: 0.1364 | val_micro_f1: 0.9488\n",
            "epoch 00268\n",
            "\ttrain_loss: 0.0429 | train_micro_f1: 0.9765\n",
            "\tval_loss: 0.1339 | val_micro_f1: 0.9513\n",
            "epoch 00269\n",
            "\ttrain_loss: 0.0396 | train_micro_f1: 0.9779\n",
            "\tval_loss: 0.1311 | val_micro_f1: 0.9522\n",
            "epoch 00270\n",
            "\ttrain_loss: 0.0384 | train_micro_f1: 0.9788\n",
            "\tval_loss: 0.1306 | val_micro_f1: 0.9528\n",
            "epoch 00271\n",
            "\ttrain_loss: 0.0348 | train_micro_f1: 0.9807\n",
            "\tval_loss: 0.1252 | val_micro_f1: 0.9550\n",
            "epoch 00272\n",
            "\ttrain_loss: 0.0299 | train_micro_f1: 0.9838\n",
            "\tval_loss: 0.1253 | val_micro_f1: 0.9561\n",
            "epoch 00273\n",
            "\ttrain_loss: 0.0260 | train_micro_f1: 0.9862\n",
            "\tval_loss: 0.1218 | val_micro_f1: 0.9584\n",
            "epoch 00274\n",
            "\ttrain_loss: 0.0221 | train_micro_f1: 0.9884\n",
            "\tval_loss: 0.1164 | val_micro_f1: 0.9602\n",
            "epoch 00275\n",
            "\ttrain_loss: 0.0184 | train_micro_f1: 0.9906\n",
            "\tval_loss: 0.1090 | val_micro_f1: 0.9637\n",
            "epoch 00276\n",
            "\ttrain_loss: 0.0151 | train_micro_f1: 0.9926\n",
            "\tval_loss: 0.1083 | val_micro_f1: 0.9651\n",
            "epoch 00277\n",
            "\ttrain_loss: 0.0138 | train_micro_f1: 0.9935\n",
            "\tval_loss: 0.1086 | val_micro_f1: 0.9655\n",
            "epoch 00278\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1064 | val_micro_f1: 0.9667\n",
            "epoch 00279\n",
            "\ttrain_loss: 0.0115 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1074 | val_micro_f1: 0.9665\n",
            "epoch 00280\n",
            "\ttrain_loss: 0.0128 | train_micro_f1: 0.9946\n",
            "\tval_loss: 0.1082 | val_micro_f1: 0.9663\n",
            "epoch 00281\n",
            "\ttrain_loss: 0.0120 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1087 | val_micro_f1: 0.9668\n",
            "epoch 00282\n",
            "\ttrain_loss: 0.0115 | train_micro_f1: 0.9952\n",
            "\tval_loss: 0.1080 | val_micro_f1: 0.9666\n",
            "epoch 00283\n",
            "\ttrain_loss: 0.0108 | train_micro_f1: 0.9953\n",
            "\tval_loss: 0.1074 | val_micro_f1: 0.9673\n",
            "epoch 00284\n",
            "\ttrain_loss: 0.0113 | train_micro_f1: 0.9951\n",
            "\tval_loss: 0.1076 | val_micro_f1: 0.9669\n",
            "epoch 00285\n",
            "\ttrain_loss: 0.0113 | train_micro_f1: 0.9949\n",
            "\tval_loss: 0.1107 | val_micro_f1: 0.9666\n",
            "epoch 00286\n",
            "\ttrain_loss: 0.0122 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1099 | val_micro_f1: 0.9661\n",
            "epoch 00287\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9951\n",
            "\tval_loss: 0.1114 | val_micro_f1: 0.9663\n",
            "epoch 00288\n",
            "\ttrain_loss: 0.0115 | train_micro_f1: 0.9950\n",
            "\tval_loss: 0.1081 | val_micro_f1: 0.9666\n",
            "epoch 00289\n",
            "\ttrain_loss: 0.0113 | train_micro_f1: 0.9950\n",
            "\tval_loss: 0.1083 | val_micro_f1: 0.9675\n",
            "epoch 00290\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1077 | val_micro_f1: 0.9667\n",
            "epoch 00291\n",
            "\ttrain_loss: 0.0112 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1107 | val_micro_f1: 0.9663\n",
            "epoch 00292\n",
            "\ttrain_loss: 0.0121 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1118 | val_micro_f1: 0.9649\n",
            "epoch 00293\n",
            "\ttrain_loss: 0.0120 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1150 | val_micro_f1: 0.9643\n",
            "epoch 00294\n",
            "\ttrain_loss: 0.0123 | train_micro_f1: 0.9942\n",
            "\tval_loss: 0.1129 | val_micro_f1: 0.9645\n",
            "epoch 00295\n",
            "\ttrain_loss: 0.0130 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1137 | val_micro_f1: 0.9649\n",
            "epoch 00296\n",
            "\ttrain_loss: 0.0135 | train_micro_f1: 0.9935\n",
            "\tval_loss: 0.1151 | val_micro_f1: 0.9643\n",
            "epoch 00297\n",
            "\ttrain_loss: 0.0128 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1137 | val_micro_f1: 0.9653\n",
            "epoch 00298\n",
            "\ttrain_loss: 0.0135 | train_micro_f1: 0.9936\n",
            "\tval_loss: 0.1138 | val_micro_f1: 0.9647\n",
            "epoch 00299\n",
            "\ttrain_loss: 0.0133 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1140 | val_micro_f1: 0.9646\n",
            "epoch 00300\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9940\n",
            "\tval_loss: 0.1151 | val_micro_f1: 0.9651\n",
            "epoch 00301\n",
            "\ttrain_loss: 0.0125 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1116 | val_micro_f1: 0.9660\n",
            "epoch 00302\n",
            "\ttrain_loss: 0.0114 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9659\n",
            "epoch 00303\n",
            "\ttrain_loss: 0.0115 | train_micro_f1: 0.9954\n",
            "\tval_loss: 0.1105 | val_micro_f1: 0.9668\n",
            "epoch 00304\n",
            "\ttrain_loss: 0.0111 | train_micro_f1: 0.9949\n",
            "\tval_loss: 0.1147 | val_micro_f1: 0.9660\n",
            "epoch 00305\n",
            "\ttrain_loss: 0.0117 | train_micro_f1: 0.9954\n",
            "\tval_loss: 0.1067 | val_micro_f1: 0.9682\n",
            "epoch 00306\n",
            "\ttrain_loss: 0.0111 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1079 | val_micro_f1: 0.9687\n",
            "epoch 00307\n",
            "\ttrain_loss: 0.0122 | train_micro_f1: 0.9953\n",
            "\tval_loss: 0.1077 | val_micro_f1: 0.9680\n",
            "epoch 00308\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1121 | val_micro_f1: 0.9673\n",
            "epoch 00309\n",
            "\ttrain_loss: 0.0126 | train_micro_f1: 0.9951\n",
            "\tval_loss: 0.1111 | val_micro_f1: 0.9670\n",
            "epoch 00310\n",
            "\ttrain_loss: 0.0122 | train_micro_f1: 0.9942\n",
            "\tval_loss: 0.1142 | val_micro_f1: 0.9657\n",
            "epoch 00311\n",
            "\ttrain_loss: 0.0150 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1157 | val_micro_f1: 0.9648\n",
            "epoch 00312\n",
            "\ttrain_loss: 0.0122 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1149 | val_micro_f1: 0.9647\n",
            "epoch 00313\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9948\n",
            "\tval_loss: 0.1132 | val_micro_f1: 0.9660\n",
            "epoch 00314\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1112 | val_micro_f1: 0.9664\n",
            "epoch 00315\n",
            "\ttrain_loss: 0.0124 | train_micro_f1: 0.9945\n",
            "\tval_loss: 0.1133 | val_micro_f1: 0.9663\n",
            "epoch 00316\n",
            "\ttrain_loss: 0.0112 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1134 | val_micro_f1: 0.9662\n",
            "epoch 00317\n",
            "\ttrain_loss: 0.0112 | train_micro_f1: 0.9950\n",
            "\tval_loss: 0.1109 | val_micro_f1: 0.9670\n",
            "epoch 00318\n",
            "\ttrain_loss: 0.0103 | train_micro_f1: 0.9953\n",
            "\tval_loss: 0.1139 | val_micro_f1: 0.9661\n",
            "epoch 00319\n",
            "\ttrain_loss: 0.0108 | train_micro_f1: 0.9953\n",
            "\tval_loss: 0.1144 | val_micro_f1: 0.9661\n",
            "epoch 00320\n",
            "\ttrain_loss: 0.0106 | train_micro_f1: 0.9952\n",
            "\tval_loss: 0.1147 | val_micro_f1: 0.9668\n",
            "epoch 00321\n",
            "\ttrain_loss: 0.0106 | train_micro_f1: 0.9956\n",
            "\tval_loss: 0.1102 | val_micro_f1: 0.9681\n",
            "epoch 00322\n",
            "\ttrain_loss: 0.0100 | train_micro_f1: 0.9956\n",
            "\tval_loss: 0.1136 | val_micro_f1: 0.9664\n",
            "epoch 00323\n",
            "\ttrain_loss: 0.0103 | train_micro_f1: 0.9956\n",
            "\tval_loss: 0.1133 | val_micro_f1: 0.9666\n",
            "epoch 00324\n",
            "\ttrain_loss: 0.0098 | train_micro_f1: 0.9957\n",
            "\tval_loss: 0.1140 | val_micro_f1: 0.9674\n",
            "epoch 00325\n",
            "\ttrain_loss: 0.0099 | train_micro_f1: 0.9958\n",
            "\tval_loss: 0.1114 | val_micro_f1: 0.9681\n",
            "epoch 00326\n",
            "\ttrain_loss: 0.0099 | train_micro_f1: 0.9959\n",
            "\tval_loss: 0.1106 | val_micro_f1: 0.9682\n",
            "epoch 00327\n",
            "\ttrain_loss: 0.0100 | train_micro_f1: 0.9954\n",
            "\tval_loss: 0.1129 | val_micro_f1: 0.9673\n",
            "epoch 00328\n",
            "\ttrain_loss: 0.0105 | train_micro_f1: 0.9955\n",
            "\tval_loss: 0.1121 | val_micro_f1: 0.9676\n",
            "epoch 00329\n",
            "\ttrain_loss: 0.0101 | train_micro_f1: 0.9955\n",
            "\tval_loss: 0.1125 | val_micro_f1: 0.9674\n",
            "epoch 00330\n",
            "\ttrain_loss: 0.0103 | train_micro_f1: 0.9952\n",
            "\tval_loss: 0.1112 | val_micro_f1: 0.9675\n",
            "epoch 00331\n",
            "\ttrain_loss: 0.0103 | train_micro_f1: 0.9954\n",
            "\tval_loss: 0.1123 | val_micro_f1: 0.9677\n",
            "epoch 00332\n",
            "\ttrain_loss: 0.0108 | train_micro_f1: 0.9950\n",
            "\tval_loss: 0.1141 | val_micro_f1: 0.9669\n",
            "epoch 00333\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9951\n",
            "\tval_loss: 0.1131 | val_micro_f1: 0.9670\n",
            "epoch 00334\n",
            "\ttrain_loss: 0.0109 | train_micro_f1: 0.9949\n",
            "\tval_loss: 0.1178 | val_micro_f1: 0.9655\n",
            "epoch 00335\n",
            "\ttrain_loss: 0.0117 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1186 | val_micro_f1: 0.9642\n",
            "epoch 00336\n",
            "\ttrain_loss: 0.0121 | train_micro_f1: 0.9942\n",
            "\tval_loss: 0.1206 | val_micro_f1: 0.9643\n",
            "epoch 00337\n",
            "\ttrain_loss: 0.0126 | train_micro_f1: 0.9947\n",
            "\tval_loss: 0.1150 | val_micro_f1: 0.9657\n",
            "epoch 00338\n",
            "\ttrain_loss: 0.0118 | train_micro_f1: 0.9943\n",
            "\tval_loss: 0.1190 | val_micro_f1: 0.9646\n",
            "epoch 00339\n",
            "\ttrain_loss: 0.0128 | train_micro_f1: 0.9941\n",
            "\tval_loss: 0.1197 | val_micro_f1: 0.9640\n",
            "epoch 00340\n",
            "\ttrain_loss: 0.0131 | train_micro_f1: 0.9933\n",
            "\tval_loss: 0.1215 | val_micro_f1: 0.9636\n",
            "epoch 00341\n",
            "\ttrain_loss: 0.0141 | train_micro_f1: 0.9931\n",
            "\tval_loss: 0.1217 | val_micro_f1: 0.9629\n",
            "epoch 00342\n",
            "\ttrain_loss: 0.0143 | train_micro_f1: 0.9925\n",
            "\tval_loss: 0.1178 | val_micro_f1: 0.9640\n",
            "epoch 00343\n",
            "\ttrain_loss: 0.0154 | train_micro_f1: 0.9924\n",
            "\tval_loss: 0.1197 | val_micro_f1: 0.9631\n",
            "epoch 00344\n",
            "\ttrain_loss: 0.0153 | train_micro_f1: 0.9921\n",
            "\tval_loss: 0.1192 | val_micro_f1: 0.9635\n",
            "epoch 00345\n",
            "\ttrain_loss: 0.0165 | train_micro_f1: 0.9920\n",
            "\tval_loss: 0.1221 | val_micro_f1: 0.9630\n",
            "epoch 00346\n",
            "\ttrain_loss: 0.0164 | train_micro_f1: 0.9915\n",
            "\tval_loss: 0.1227 | val_micro_f1: 0.9627\n",
            "epoch 00347\n",
            "\ttrain_loss: 0.0168 | train_micro_f1: 0.9918\n",
            "\tval_loss: 0.1201 | val_micro_f1: 0.9630\n",
            "epoch 00348\n",
            "\ttrain_loss: 0.0158 | train_micro_f1: 0.9919\n",
            "\tval_loss: 0.1209 | val_micro_f1: 0.9635\n",
            "epoch 00349\n",
            "\ttrain_loss: 0.0159 | train_micro_f1: 0.9920\n",
            "\tval_loss: 0.1200 | val_micro_f1: 0.9636\n",
            "epoch 00350\n",
            "\ttrain_loss: 0.0172 | train_micro_f1: 0.9912\n",
            "\tval_loss: 0.1226 | val_micro_f1: 0.9634\n",
            "epoch 00351\n",
            "\ttrain_loss: 0.0179 | train_micro_f1: 0.9909\n",
            "\tval_loss: 0.1250 | val_micro_f1: 0.9611\n",
            "epoch 00352\n",
            "\ttrain_loss: 0.0212 | train_micro_f1: 0.9891\n",
            "\tval_loss: 0.1266 | val_micro_f1: 0.9606\n",
            "epoch 00353\n",
            "\ttrain_loss: 0.0238 | train_micro_f1: 0.9880\n",
            "\tval_loss: 0.1260 | val_micro_f1: 0.9598\n",
            "epoch 00354\n",
            "\ttrain_loss: 0.0228 | train_micro_f1: 0.9884\n",
            "\tval_loss: 0.1264 | val_micro_f1: 0.9603\n",
            "epoch 00355\n",
            "\ttrain_loss: 0.0252 | train_micro_f1: 0.9874\n",
            "\tval_loss: 0.1315 | val_micro_f1: 0.9593\n",
            "epoch 00356\n",
            "\ttrain_loss: 0.0283 | train_micro_f1: 0.9859\n",
            "\tval_loss: 0.1284 | val_micro_f1: 0.9599\n",
            "epoch 00357\n",
            "\ttrain_loss: 0.0258 | train_micro_f1: 0.9867\n",
            "\tval_loss: 0.1306 | val_micro_f1: 0.9585\n",
            "epoch 00358\n",
            "\ttrain_loss: 0.0330 | train_micro_f1: 0.9830\n",
            "\tval_loss: 0.1469 | val_micro_f1: 0.9505\n",
            "epoch 00359\n",
            "\ttrain_loss: 0.0560 | train_micro_f1: 0.9721\n",
            "\tval_loss: 0.1506 | val_micro_f1: 0.9479\n",
            "epoch 00360\n",
            "\ttrain_loss: 0.0619 | train_micro_f1: 0.9685\n",
            "\tval_loss: 0.1587 | val_micro_f1: 0.9439\n",
            "epoch 00361\n",
            "\ttrain_loss: 0.0502 | train_micro_f1: 0.9736\n",
            "\tval_loss: 0.1483 | val_micro_f1: 0.9496\n",
            "epoch 00362\n",
            "\ttrain_loss: 0.0453 | train_micro_f1: 0.9760\n",
            "\tval_loss: 0.1427 | val_micro_f1: 0.9513\n",
            "epoch 00363\n",
            "\ttrain_loss: 0.0437 | train_micro_f1: 0.9769\n",
            "\tval_loss: 0.1489 | val_micro_f1: 0.9479\n",
            "epoch 00364\n",
            "\ttrain_loss: 0.0411 | train_micro_f1: 0.9784\n",
            "\tval_loss: 0.1375 | val_micro_f1: 0.9526\n",
            "epoch 00365\n",
            "\ttrain_loss: 0.0368 | train_micro_f1: 0.9809\n",
            "\tval_loss: 0.1323 | val_micro_f1: 0.9557\n",
            "epoch 00366\n",
            "\ttrain_loss: 0.0339 | train_micro_f1: 0.9828\n",
            "\tval_loss: 0.1378 | val_micro_f1: 0.9565\n",
            "epoch 00367\n",
            "\ttrain_loss: 0.0328 | train_micro_f1: 0.9833\n",
            "\tval_loss: 0.1282 | val_micro_f1: 0.9586\n",
            "epoch 00368\n",
            "\ttrain_loss: 0.0285 | train_micro_f1: 0.9851\n",
            "\tval_loss: 0.1288 | val_micro_f1: 0.9592\n",
            "epoch 00369\n",
            "\ttrain_loss: 0.0270 | train_micro_f1: 0.9860\n",
            "\tval_loss: 0.1262 | val_micro_f1: 0.9595\n",
            "epoch 00370\n",
            "\ttrain_loss: 0.0269 | train_micro_f1: 0.9855\n",
            "\tval_loss: 0.1316 | val_micro_f1: 0.9579\n",
            "epoch 00371\n",
            "\ttrain_loss: 0.0275 | train_micro_f1: 0.9855\n",
            "\tval_loss: 0.1226 | val_micro_f1: 0.9604\n",
            "epoch 00372\n",
            "\ttrain_loss: 0.0246 | train_micro_f1: 0.9866\n",
            "\tval_loss: 0.1234 | val_micro_f1: 0.9610\n",
            "epoch 00373\n",
            "\ttrain_loss: 0.0250 | train_micro_f1: 0.9874\n",
            "\tval_loss: 0.1251 | val_micro_f1: 0.9604\n",
            "epoch 00374\n",
            "\ttrain_loss: 0.0214 | train_micro_f1: 0.9889\n",
            "\tval_loss: 0.1233 | val_micro_f1: 0.9625\n",
            "epoch 00375\n",
            "\ttrain_loss: 0.0196 | train_micro_f1: 0.9906\n",
            "\tval_loss: 0.1182 | val_micro_f1: 0.9641\n",
            "epoch 00376\n",
            "\ttrain_loss: 0.0144 | train_micro_f1: 0.9928\n",
            "\tval_loss: 0.1169 | val_micro_f1: 0.9654\n",
            "epoch 00377\n",
            "\ttrain_loss: 0.0129 | train_micro_f1: 0.9939\n",
            "\tval_loss: 0.1137 | val_micro_f1: 0.9669\n",
            "epoch 00378\n",
            "\ttrain_loss: 0.0115 | train_micro_f1: 0.9944\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9679\n",
            "epoch 00379\n",
            "\ttrain_loss: 0.0111 | train_micro_f1: 0.9952\n",
            "\tval_loss: 0.1108 | val_micro_f1: 0.9688\n",
            "epoch 00380\n",
            "\ttrain_loss: 0.0096 | train_micro_f1: 0.9954\n",
            "\tval_loss: 0.1103 | val_micro_f1: 0.9692\n",
            "epoch 00381\n",
            "\ttrain_loss: 0.0099 | train_micro_f1: 0.9959\n",
            "\tval_loss: 0.1086 | val_micro_f1: 0.9703\n",
            "epoch 00382\n",
            "\ttrain_loss: 0.0096 | train_micro_f1: 0.9959\n",
            "\tval_loss: 0.1113 | val_micro_f1: 0.9692\n",
            "epoch 00383\n",
            "\ttrain_loss: 0.0100 | train_micro_f1: 0.9959\n",
            "\tval_loss: 0.1116 | val_micro_f1: 0.9697\n",
            "epoch 00384\n",
            "\ttrain_loss: 0.0100 | train_micro_f1: 0.9959\n",
            "\tval_loss: 0.1101 | val_micro_f1: 0.9697\n",
            "epoch 00385\n",
            "\ttrain_loss: 0.0099 | train_micro_f1: 0.9959\n",
            "\tval_loss: 0.1125 | val_micro_f1: 0.9695\n",
            "epoch 00386\n",
            "\ttrain_loss: 0.0102 | train_micro_f1: 0.9961\n",
            "\tval_loss: 0.1081 | val_micro_f1: 0.9698\n",
            "epoch 00387\n",
            "\ttrain_loss: 0.0103 | train_micro_f1: 0.9957\n",
            "\tval_loss: 0.1142 | val_micro_f1: 0.9693\n",
            "epoch 00388\n",
            "\ttrain_loss: 0.0112 | train_micro_f1: 0.9956\n",
            "\tval_loss: 0.1094 | val_micro_f1: 0.9699\n",
            "epoch 00389\n",
            "\ttrain_loss: 0.0116 | train_micro_f1: 0.9953\n",
            "\tval_loss: 0.1119 | val_micro_f1: 0.9693\n",
            "epoch 00390\n",
            "\ttrain_loss: 0.0105 | train_micro_f1: 0.9958\n",
            "\tval_loss: 0.1096 | val_micro_f1: 0.9697\n",
            "epoch 00391\n",
            "\ttrain_loss: 0.0100 | train_micro_f1: 0.9956\n",
            "\tval_loss: 0.1133 | val_micro_f1: 0.9693\n",
            "epoch 00392\n",
            "\ttrain_loss: 0.0108 | train_micro_f1: 0.9962\n",
            "\tval_loss: 0.1119 | val_micro_f1: 0.9690\n",
            "epoch 00393\n",
            "\ttrain_loss: 0.0111 | train_micro_f1: 0.9952\n",
            "\tval_loss: 0.1174 | val_micro_f1: 0.9683\n",
            "epoch 00394\n",
            "\ttrain_loss: 0.0115 | train_micro_f1: 0.9959\n",
            "\tval_loss: 0.1124 | val_micro_f1: 0.9697\n",
            "epoch 00395\n",
            "\ttrain_loss: 0.0096 | train_micro_f1: 0.9956\n",
            "\tval_loss: 0.1139 | val_micro_f1: 0.9692\n",
            "epoch 00396\n",
            "\ttrain_loss: 0.0102 | train_micro_f1: 0.9961\n",
            "\tval_loss: 0.1146 | val_micro_f1: 0.9690\n",
            "epoch 00397\n",
            "\ttrain_loss: 0.0093 | train_micro_f1: 0.9960\n",
            "\tval_loss: 0.1153 | val_micro_f1: 0.9687\n",
            "epoch 00398\n",
            "\ttrain_loss: 0.0096 | train_micro_f1: 0.9963\n",
            "\tval_loss: 0.1155 | val_micro_f1: 0.9688\n",
            "epoch 00399\n",
            "\ttrain_loss: 0.0095 | train_micro_f1: 0.9962\n",
            "\tval_loss: 0.1112 | val_micro_f1: 0.9701\n",
            "best model performance @ epoch 00195: \n",
            "\tval_loss: 0.1013 | val_micro_f1: 0.9623\n",
            "\ttest_loss: 0.0582 | test_micro_f1: 0.9765\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_759c4959-e3e8-4b93-b39f-eac6656c2b59\", \"GATv1PPI\", 29569305)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "best_model, _, _, _, _, model_saver = train(model, ppi_train_params)\n",
        "best_model = model\n",
        "model_saver.download_best_model_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHIRZuPl4f6o",
        "outputId": "4ae3f66f-b67d-41aa-8166-04ab175acabc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataBatch(x=[5524, 50], edge_index=[2, 167500], y=[5524, 121], batch=[5524], ptr=[3])\n",
            "DiGraph with 5524 nodes and 167500 edges\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "[test_graph] = list(dataset.test_loader)\n",
        "test_graph_nx = to_networkx(test_graph)\n",
        "\n",
        "print(test_graph)\n",
        "print(test_graph_nx)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.0 ('grl-env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0 (default, Mar 18 2022, 03:31:43) \n[Clang 12.0.1 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "4737062b79cbd82e7f605897a0c10fa5613d72f803a33da3548714c1d9e40d67"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
